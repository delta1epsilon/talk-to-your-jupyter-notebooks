{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with indexing Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from langchain_community.document_loaders import NotebookLoader\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_notebooks_docs(folder_path):\n",
    "    docs = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.ipynb'):\n",
    "            loader = NotebookLoader(\n",
    "                path=os.path.join(folder_path, file_name), \n",
    "                include_outputs=True,\n",
    "                max_output_length=500,\n",
    "                traceback=False\n",
    "            )\n",
    "            docs.append(loader.load()[0])\n",
    "    return docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_notebooks_docs('./notebooks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\'markdown\\' cell: \\'[\\'Notebook Source: https://www.kaggle.com/code/alexisbcook/titanic-tutorial/notebook\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'---\\']\\'\\n\\n \\'markdown\\' cell: \\'[\"Logging into Kaggle for the first time can be daunting. Our competitions often have large cash prizes, public leaderboards, and involve complex data. Nevertheless, we really think all data scientists can rapidly learn from machine learning competitions and meaningfully contribute to our community. To give you a clear understanding of how our platform works and a mental model of the type of learning you could do on Kaggle, we\\'ve created a Getting Started tutorial for the Titanic competition. It walks you through the initial steps required to get your first decent submission on the leaderboard. By the end of the tutorial, you\\'ll also have a solid understanding of how to use Kaggle\\'s online coding environment, where you\\'ll have trained your own machine learning model.\\\\n\", \\'\\\\n\\', \\'So if this is your first time entering a Kaggle competition, regardless of whether you:\\\\n\\', \\'- have experience with handling large datasets,\\\\n\\', \"- haven\\'t done much coding,\\\\n\", \\'- are newer to data science, or\\\\n\\', \"- are relatively experienced (but are just unfamiliar with Kaggle\\'s platform),\\\\n\", \\'\\\\n\\', \"you\\'re in the right place! \\\\n\", \\'\\\\n\\', \\'# Part 1: Get started\\\\n\\', \\'\\\\n\\', \"In this section, you\\'ll learn more about the competition and make your first submission. \\\\n\", \\'\\\\n\\', \\'## Join the competition!\\\\n\\', \\'\\\\n\\', \\'The first thing to do is to join the competition!  Open a new window with **[the competition page](https://www.kaggle.com/c/titanic)**, and click on the **\"Join Competition\"** button, if you haven\\\\\\'t already.  (_If you see a \"Submit Predictions\" button instead of a \"Join Competition\" button, you have already joined the competition, and don\\\\\\'t need to do so again._)\\\\n\\', \\'\\\\n\\', \\'![](https://i.imgur.com/07cskyU.png)\\\\n\\', \\'\\\\n\\', \\'This takes you to the rules acceptance page.  You must accept the competition rules in order to participate.  These rules govern how many submissions you can make per day, the maximum team size, and other competition-specific details.   Then, click on **\"I Understand and Accept\"** to indicate that you will abide by the competition rules.\\\\n\\', \\'\\\\n\\', \\'## The challenge\\\\n\\', \\'\\\\n\\', \\'The competition is simple: we want you to use the Titanic passenger data (name, age, price of ticket, etc) to try to predict who will survive and who will die.\\\\n\\', \\'\\\\n\\', \\'## The data\\\\n\\', \\'\\\\n\\', \\'To take a look at the competition data, click on the **<a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\" rel=\"noopener noreferrer\"><b>Data tab</b></a>** at the top of the competition page.  Then, scroll down to find the list of files.  \\\\n\\', \\'There are three files in the data: (1) **train.csv**, (2) **test.csv**, and (3) **gender_submission.csv**.\\\\n\\', \\'\\\\n\\', \\'### (1) train.csv\\\\n\\', \\'\\\\n\\', \"**train.csv** contains the details of a subset of the passengers on board (891 passengers, to be exact -- where each passenger gets a different row in the table).  To investigate this data, click on the name of the file on the left of the screen.  Once you\\'ve done this, you can view all of the data in the window.  \\\\n\", \\'\\\\n\\', \\'![](https://i.imgur.com/cYsdt0n.png)\\\\n\\', \\'\\\\n\\', \\'The values in the second column (**\"Survived\"**) can be used to determine whether each passenger survived or not: \\\\n\\', \\'- if it\\\\\\'s a \"1\", the passenger survived.\\\\n\\', \\'- if it\\\\\\'s a \"0\", the passenger died.\\\\n\\', \\'\\\\n\\', \\'For instance, the first passenger listed in **train.csv** is Mr. Owen Harris Braund.  He was 22 years old when he died on the Titanic.\\\\n\\', \\'\\\\n\\', \\'### (2) test.csv\\\\n\\', \\'\\\\n\\', \\'Using the patterns you find in **train.csv**, you have to predict whether the other 418 passengers on board (in **test.csv**) survived.  \\\\n\\', \\'\\\\n\\', \\'Click on **test.csv** (on the left of the screen) to examine its contents.  Note that **test.csv** does not have a **\"Survived\"** column - this information is hidden from you, and how well you do at predicting these hidden values will determine how highly you score in the competition! \\\\n\\', \\'\\\\n\\', \\'### (3) gender_submission.csv\\\\n\\', \\'\\\\n\\', \\'The **gender_submission.csv** file is provided as an example that shows how you should structure your predictions.  It predicts that all female passengers survived, and all male passengers died.  Your hypotheses regarding survival will probably be different, which will lead to a different submission file.  But, just like this file, your submission should have:\\\\n\\', \\'- a **\"PassengerId\"** column containing the IDs of each passenger from **test.csv**.\\\\n\\', \\'- a **\"Survived\"** column (that you will create!) with a \"1\" for the rows where you think the passenger survived, and a \"0\" where you predict that the passenger died.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Part 2: Your coding environment\\\\n\\', \\'\\\\n\\', \"In this section, you\\'ll train your own machine learning model to improve your predictions.  _If you\\'ve never written code before or don\\'t have any experience with machine learning, don\\'t worry!  We don\\'t assume any prior experience in this tutorial._\\\\n\", \\'\\\\n\\', \\'## The Notebook\\\\n\\', \\'\\\\n\\', \"The first thing to do is to create a Kaggle Notebook where you\\'ll store all of your code.  You can use Kaggle Notebooks to getting up and running with writing code quickly, and without having to install anything on your computer.  (_If you are interested in deep learning, we also offer free GPU access!_) \\\\n\", \\'\\\\n\\', \\'Begin by clicking on the **<a href=\"https://www.kaggle.com/c/titanic/kernels\" target=\"_blank\">Code tab</a>** on the competition page.  Then, click on **\"New Notebook\"**.\\\\n\\', \\'\\\\n\\', \\'![](https://i.imgur.com/v2i82Xd.png)\\\\n\\', \\'\\\\n\\', \\'Your notebook will take a few seconds to load.  In the top left corner, you can see the name of your notebook -- something like **\"kernel2daed3cd79\"**.\\\\n\\', \\'\\\\n\\', \\'![](https://i.imgur.com/64ZFT1L.png)\\\\n\\', \\'\\\\n\\', \\'You can edit the name by clicking on it.  Change it to something more descriptive, like **\"Getting Started with Titanic\"**.  \\\\n\\', \\'\\\\n\\', \\'![](https://i.imgur.com/uwyvzXq.png)\\\\n\\', \\'\\\\n\\', \\'## Your first lines of code\\\\n\\', \\'\\\\n\\', \\'When you start a new notebook, it has two gray boxes for storing code.  We refer to these gray boxes as \"code cells\".\\\\n\\', \\'\\\\n\\', \\'![](https://i.imgur.com/q9mwkZM.png)\\\\n\\', \\'\\\\n\\', \"The first code cell already has some code in it.  To run this code, put your cursor in the code cell.  (_If your cursor is in the right place, you\\'ll notice a blue vertical line to the left of the gray box._)  Then, either hit the play button (which appears to the left of the blue line), or hit **[Shift] + [Enter]** on your keyboard.\\\\n\", \\'\\\\n\\', \\'If the code runs successfully, three lines of output are returned.  Below, you can see the same code that you just ran, along with the output that you should see in your notebook.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'# This Python 3 environment comes with many helpful analytics libraries installed\\\\n\\', \\'# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\\\\n\\', \"# For example, here\\'s several helpful packages to load in \\\\n\", \\'\\\\n\\', \\'import numpy as np # linear algebra\\\\n\\', \\'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\\\\n\\', \\'\\\\n\\', \\'# Input data files are available in the \"../input/\" directory.\\\\n\\', \\'# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\\\\n\\', \\'\\\\n\\', \\'import os\\\\n\\', \"for dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\\\n\", \\'    for filename in filenames:\\\\n\\', \\'        print(os.path.join(dirname, filename))\\\\n\\', \\'\\\\n\\', \\'# Any results you write to the current directory are saved as output.\\']\\'\\n with output: \\'[\\'/kaggle/input/titanic/train.csv\\\\n\\', \\'/kaggle/input/titanic/test.csv\\\\n\\', \\'/kaggle/input/titanic/gender_submission.csv\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\"This shows us where the competition data is stored, so that we can load the files into the notebook.  We\\'ll do that next.\\\\n\", \\'\\\\n\\', \\'## Load the data\\\\n\\', \\'\\\\n\\', \\'The second code cell in your notebook now appears below the three lines of output with the file locations.\\\\n\\', \\'\\\\n\\', \\'![](https://i.imgur.com/OQBax9n.png)\\\\n\\', \\'\\\\n\\', \"Type the two lines of code below into your second code cell.  Then, once you\\'re done, either click on the blue play button, or hit **[Shift] + [Enter]**.  \"]\\'\\n\\n  \\'markdown\\' cell: \\'[\"Your code should return the output above, which corresponds to the first five rows of the table in **train.csv**.  It\\'s very important that you see this output **in your notebook** before proceeding with the tutorial!\\\\n\", \\'> _If your code does not produce this output_, double-check that your code is identical to the two lines above.  And, make sure your cursor is in the code cell before hitting **[Shift] + [Enter]**.\\\\n\\', \\'\\\\n\\', \\'The code that you\\\\\\'ve just written is in the Python programming language. It uses a Python \"module\" called **pandas** (abbreviated as `pd`) to load the table from the **train.csv** file into the notebook. To do this, we needed to plug in the location of the file (which we saw was `/kaggle/input/titanic/train.csv`).  \\\\n\\', \"> If you\\'re not already familiar with Python (and pandas), the code shouldn\\'t make sense to you -- but don\\'t worry!  The point of this tutorial is to (quickly!) make your first submission to the competition.  At the end of the tutorial, we suggest resources to continue your learning.\\\\n\", \\'\\\\n\\', \\'At this point, you should have at least three code cells in your notebook.  \\\\n\\', \\'![](https://i.imgur.com/ReLhYca.png)\\\\n\\', \\'\\\\n\\', \"Copy the code below into the third code cell of your notebook to load the contents of the **test.csv** file.  Don\\'t forget to click on the play button (or hit **[Shift] + [Enter]**)!\"]\\'\\n\\n  \\'markdown\\' cell: \\'[\\'As before, make sure that you see the output above in your notebook before continuing.  \\\\n\\', \\'\\\\n\\', \\'Once all of the code runs successfully, all of the data (in **train.csv** and **test.csv**) is loaded in the notebook.  (_The code above shows only the first 5 rows of each table, but all of the data is there -- all 891 rows of **train.csv** and all 418 rows of **test.csv**!_)\\\\n\\', \\'\\\\n\\', \\'# Part 3: Your first submission\\\\n\\', \\'\\\\n\\', \\'Remember our goal: we want to find patterns in **train.csv** that help us predict whether the passengers in **test.csv** survived.\\\\n\\', \\'\\\\n\\', \"It might initially feel overwhelming to look for patterns, when there\\'s so much data to sort through.  So, we\\'ll start simple.\\\\n\", \\'\\\\n\\', \\'## Explore a pattern\\\\n\\', \\'\\\\n\\', \\'Remember that the sample submission file in **gender_submission.csv** assumes that all female passengers survived (and all male passengers died).  \\\\n\\', \\'\\\\n\\', \"Is this a reasonable first guess?  We\\'ll check if this pattern holds true in the data (in **train.csv**).\\\\n\", \\'\\\\n\\', \\'Copy the code below into a new code cell.  Then, run the cell.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'women = train_data.loc[train_data.Sex == \\\\\\'female\\\\\\'][\"Survived\"]\\\\n\\', \\'rate_women = sum(women)/len(women)\\\\n\\', \\'\\\\n\\', \\'print(\"% of women who survived:\", rate_women)\\']\\'\\n with output: \\'[\\'% of women who survived: 0.7420382165605095\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Before moving on, make sure that your code returns the output above.  The code above calculates the percentage of female passengers (in **train.csv**) who survived.\\\\n\\', \\'\\\\n\\', \\'Then, run the code below in another code cell:\\']\\'\\n\\n \\'code\\' cell: \\'[\\'men = train_data.loc[train_data.Sex == \\\\\\'male\\\\\\'][\"Survived\"]\\\\n\\', \\'rate_men = sum(men)/len(men)\\\\n\\', \\'\\\\n\\', \\'print(\"% of men who survived:\", rate_men)\\']\\'\\n with output: \\'[\\'% of men who survived: 0.18890814558058924\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'The code above calculates the percentage of male passengers (in **train.csv**) who survived.\\\\n\\', \\'\\\\n\\', \\'From this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in **gender_submission.csv** is not a bad first guess!\\\\n\\', \\'\\\\n\\', \"But at the end of the day, this gender-based submission bases its predictions on only a single column.  As you can imagine, by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions.  Since it is quite difficult to consider several columns at once (or, it would take a long time to consider all possible patterns in many different columns simultaneously), we\\'ll use machine learning to automate this for us.\\\\n\", \\'\\\\n\\', \\'## Your first machine learning model\\\\n\\', \\'\\\\n\\', \\'We\\\\\\'ll build what\\\\\\'s known as a **random forest model**.  This model is constructed of several \"trees\" (there are three trees in the picture below, but we\\\\\\'ll construct 100!) that will individually consider each passenger\\\\\\'s data and vote on whether the individual survived.  Then, the random forest model makes a democratic decision: the outcome with the most votes wins!\\\\n\\', \\'\\\\n\\', \\'![](https://i.imgur.com/AC9Bq63.png)\\\\n\\', \\'\\\\n\\', \\'The code cell below looks for patterns in four different columns (**\"Pclass\"**, **\"Sex\"**, **\"SibSp\"**, and **\"Parch\"**) of the data.  It constructs the trees in the random forest model based on patterns in the **train.csv** file, before generating predictions for the passengers in **test.csv**.  The code also saves these new predictions in a CSV file **submission.csv**.\\\\n\\', \\'\\\\n\\', \\'Copy this code into your notebook, and run it in a new code cell.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'from sklearn.ensemble import RandomForestClassifier\\\\n\\', \\'\\\\n\\', \\'y = train_data[\"Survived\"]\\\\n\\', \\'\\\\n\\', \\'features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\\\\n\\', \\'X = pd.get_dummies(train_data[features])\\\\n\\', \\'X_test = pd.get_dummies(test_data[features])\\\\n\\', \\'\\\\n\\', \\'model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\\\\n\\', \\'model.fit(X, y)\\\\n\\', \\'predictions = model.predict(X_test)\\\\n\\', \\'\\\\n\\', \"output = pd.DataFrame({\\'PassengerId\\': test_data.PassengerId, \\'Survived\\': predictions})\\\\n\", \"output.to_csv(\\'submission.csv\\', index=False)\\\\n\", \\'print(\"Your submission was successfully saved!\")\\']\\'\\n with output: \\'[\\'Your submission was successfully saved!\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Make sure that your notebook outputs the same message above (`Your submission was successfully saved!`) before moving on.\\\\n\\', \"> Again, don\\'t worry if this code doesn\\'t make sense to you!  For now, we\\'ll focus on how to generate and submit predictions.\\\\n\", \\'\\\\n\\', \\'Once you\\\\\\'re ready, click on the **\"Save Version\"** button in the top right corner of your notebook.  This will generate a pop-up window.  \\\\n\\', \\'- Ensure that the **\"Save and Run All\"** option is selected, and then click on the **\"Save\"** button.\\\\n\\', \\'- This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **\"Save Version\"** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  \\\\n\\', \\'- Click on the **Data** tab on the top of the screen.  Then, click on the **\"Submit\"** button to submit your results.\\\\n\\', \\'\\\\n\\', \\'![](https://i.imgur.com/1ocaUl4.png)\\\\n\\', \\'\\\\n\\', \\'Congratulations for making your first submission to a Kaggle competition!  Within ten minutes, you should receive a message providing your spot on the leaderboard.  Great work!\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Part 4: Learn more!\\\\n\\', \\'\\\\n\\', \"If you\\'re interested in learning more, we strongly suggest our (3-hour) **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)** course, which will help you fully understand all of the code that we\\'ve presented here.  You\\'ll also know enough to generate even better predictions!\"]\\'\\n\\n', metadata={'source': 'notebooks/titanic-tutorial.ipynb'}),\n",
       " Document(page_content='\\'markdown\\' cell: \\'[\\'Notebook Source: https://www.kaggle.com/code/ash316/eda-to-prediction-dietanic \\\\n\\', \\'\\\\n\\', \\'---\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# EDA To Prediction (DieTanic)\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### *Sometimes life has a cruel sense of humor, giving you the thing you always wanted at the worst time possible.*\\\\n\\', \\'                                                                                       -Lisa Kleypas\\\\n\\', \\'\\\\n\\', \\'                                                                                                                                     \\']\\'\\n\\n \\'markdown\\' cell: \\'[\"The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. That\\'s why the name **DieTanic**.  This is a very unforgetable disaster that no one in the world can forget.\\\\n\", \\'\\\\n\\', \\'It took about $7.5 million to build the Titanic and it sunk under the ocean due to collision. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.\\\\n\\', \\'\\\\n\\', \\'The Objective of this notebook is to give an **idea how is the workflow in any predictive modeling problem**. How do we check features, how do we add new features and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it.\\\\n\\', \\'\\\\n\\', \\'If You Like the notebook and think that it helped you..**PLEASE UPVOTE**. It will keep me motivated.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Contents of the Notebook:\\\\n\\', \\'\\\\n\\', \\'#### Part1: Exploratory Data Analysis(EDA):\\\\n\\', \\'1)Analysis of the features.\\\\n\\', \\'\\\\n\\', \\'2)Finding any relations or trends considering multiple features.\\\\n\\', \\'#### Part2: Feature Engineering and Data Cleaning:\\\\n\\', \\'1)Adding any few features.\\\\n\\', \\'\\\\n\\', \\'2)Removing redundant features.\\\\n\\', \\'\\\\n\\', \\'3)Converting features into suitable form for modeling.\\\\n\\', \\'#### Part3: Predictive Modeling\\\\n\\', \\'1)Running Basic Algorithms.\\\\n\\', \\'\\\\n\\', \\'2)Cross Validation.\\\\n\\', \\'\\\\n\\', \\'3)Ensembling.\\\\n\\', \\'\\\\n\\', \\'4)Important Features Extraction.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Part1: Exploratory Data Analysis(EDA)\\']\\'\\n\\n \\'code\\' cell: \\'[\\'import numpy as np \\\\n\\', \\'import pandas as pd\\\\n\\', \\'import matplotlib.pyplot as plt\\\\n\\', \\'import seaborn as sns\\\\n\\', \"plt.style.use(\\'fivethirtyeight\\')\\\\n\", \\'import warnings\\\\n\\', \"warnings.filterwarnings(\\'ignore\\')\\\\n\", \\'%matplotlib inline\\']\\'\\n\\n \\'code\\' cell: \\'[\"data=pd.read_csv(\\'../input/train.csv\\')\"]\\'\\n\\n \\'code\\' cell: \\'[\\'data.head()\\']\\'\\n\\n \\'code\\' cell: \\'[\\'data.isnull().sum() #checking for total null values\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'The **Age, Cabin and Embarked** have null values. I will try to fix them.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### How many Survived??\\']\\'\\n\\n \\'code\\' cell: \\'[\\'f,ax=plt.subplots(1,2,figsize=(18,8))\\\\n\\', \"data[\\'Survived\\'].value_counts().plot.pie(explode=[0,0.1],autopct=\\'%1.1f%%\\',ax=ax[0],shadow=True)\\\\n\", \"ax[0].set_title(\\'Survived\\')\\\\n\", \"ax[0].set_ylabel(\\'\\')\\\\n\", \"sns.countplot(\\'Survived\\',data=data,ax=ax[1])\\\\n\", \"ax[1].set_title(\\'Survived\\')\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'It is evident that not many passengers survived the accident. \\\\n\\', \\'\\\\n\\', \"Out of 891 passengers in training set, only around 350 survived i.e Only **38.4%** of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn\\'t.\\\\n\", \\'\\\\n\\', \\'We will try to check the survival rate by using the different features of the dataset. Some of the features being Sex, Port Of Embarcation, Age,etc.\\\\n\\', \\'\\\\n\\', \\'First let us understand the different types of features.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Types Of Features\\\\n\\', \\'\\\\n\\', \\'### Categorical Features:\\\\n\\', \\'A categorical variable is one that has two or more categories and each value in that feature can be categorised by them.For example, gender is a categorical variable having two categories (male and female). Now we cannot sort or give any ordering to such variables. They are also known as **Nominal Variables**.\\\\n\\', \\'\\\\n\\', \\'**Categorical Features in the dataset: Sex,Embarked.**\\\\n\\', \\'\\\\n\\', \\'### Ordinal Features:\\\\n\\', \\'An ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: If we have a feature like **Height** with values **Tall, Medium, Short**, then Height is a ordinal variable. Here we can have a relative sort in the variable.\\\\n\\', \\'\\\\n\\', \\'**Ordinal Features in the dataset: PClass**\\\\n\\', \\'\\\\n\\', \\'### Continous Feature:\\\\n\\', \\'A feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column.\\\\n\\', \\'\\\\n\\', \\'**Continous Features in the dataset: Age**\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Analysing The Features\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Sex--> Categorical Feature\\']\\'\\n\\n \\'code\\' cell: \\'[\"data.groupby([\\'Sex\\',\\'Survived\\'])[\\'Survived\\'].count()\"]\\'\\n\\n \\'code\\' cell: \\'[\\'f,ax=plt.subplots(1,2,figsize=(18,8))\\\\n\\', \"data[[\\'Sex\\',\\'Survived\\']].groupby([\\'Sex\\']).mean().plot.bar(ax=ax[0])\\\\n\", \"ax[0].set_title(\\'Survived vs Sex\\')\\\\n\", \"sns.countplot(\\'Sex\\',hue=\\'Survived\\',data=data,ax=ax[1])\\\\n\", \"ax[1].set_title(\\'Sex:Survived vs Dead\\')\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'This looks interesting. The number of men on the ship is lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survival rates for a **women on the ship is around 75% while that for men in around 18-19%.**\\\\n\\', \\'\\\\n\\', \\'This looks to be a **very important** feature for modeling. But is it the best??   Lets check other features.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Pclass --> Ordinal Feature\\']\\'\\n\\n \\'code\\' cell: \\'[\"pd.crosstab(data.Pclass,data.Survived,margins=True).style.background_gradient(cmap=\\'summer_r\\')\"]\\'\\n\\n \\'code\\' cell: \\'[\\'f,ax=plt.subplots(1,2,figsize=(18,8))\\\\n\\', \"data[\\'Pclass\\'].value_counts().plot.bar(color=[\\'#CD7F32\\',\\'#FFDF00\\',\\'#D3D3D3\\'],ax=ax[0])\\\\n\", \"ax[0].set_title(\\'Number Of Passengers By Pclass\\')\\\\n\", \"ax[0].set_ylabel(\\'Count\\')\\\\n\", \"sns.countplot(\\'Pclass\\',hue=\\'Survived\\',data=data,ax=ax[1])\\\\n\", \"ax[1].set_title(\\'Pclass:Survived vs Dead\\')\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\"People say **Money Can\\'t Buy Everything**. But we can clearly see that Passenegers Of Pclass 1 were given a very high priority while rescue. Even though the the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somewhere around **25%**.\\\\n\", \\'\\\\n\\', \\'For Pclass 1 %survived is around **63%** while for Pclass2 is around **48%**. So money and status matters. Such a materialistic world.\\\\n\\', \\'\\\\n\\', \\'Lets Dive in little bit more and check for other interesting observations. Lets check survival rate with **Sex and Pclass** Together.\\']\\'\\n\\n \\'code\\' cell: \\'[\"pd.crosstab([data.Sex,data.Survived],data.Pclass,margins=True).style.background_gradient(cmap=\\'summer_r\\')\"]\\'\\n\\n \\'code\\' cell: \\'[\"sns.factorplot(\\'Pclass\\',\\'Survived\\',hue=\\'Sex\\',data=data)\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'We use **FactorPlot** in this case, because they make the seperation of categorical values easy.\\\\n\\', \\'\\\\n\\', \\'Looking at the **CrossTab** and the **FactorPlot**, we can easily infer that survival for **Women from Pclass1** is about **95-96%**, as only 3 out of 94 Women from Pclass1 died. \\\\n\\', \\'\\\\n\\', \\'It is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.\\\\n\\', \\'\\\\n\\', \\'Looks like Pclass is also an important feature. Lets analyse other features.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Age--> Continous Feature\\\\n\\']\\'\\n\\n \\'code\\' cell: \\'[\"print(\\'Oldest Passenger was of:\\',data[\\'Age\\'].max(),\\'Years\\')\\\\n\", \"print(\\'Youngest Passenger was of:\\',data[\\'Age\\'].min(),\\'Years\\')\\\\n\", \"print(\\'Average Age on the ship:\\',data[\\'Age\\'].mean(),\\'Years\\')\"]\\'\\n\\n \\'code\\' cell: \\'[\\'f,ax=plt.subplots(1,2,figsize=(18,8))\\\\n\\', \\'sns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[0])\\\\n\\', \"ax[0].set_title(\\'Pclass and Age vs Survived\\')\\\\n\", \\'ax[0].set_yticks(range(0,110,10))\\\\n\\', \\'sns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[1])\\\\n\\', \"ax[1].set_title(\\'Sex and Age vs Survived\\')\\\\n\", \\'ax[1].set_yticks(range(0,110,10))\\\\n\\', \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'#### Observations:\\\\n\\', \\'\\\\n\\', \\'1)The number of children increases with Pclass and the survival rate for passenegers below Age 10(i.e children) looks to be good irrespective of the Pclass.\\\\n\\', \\'\\\\n\\', \\'2)Survival chances for Passenegers aged 20-50 from Pclass1 is high and is even better for Women.\\\\n\\', \\'\\\\n\\', \\'3)For males, the survival chances decreases with an increase in age.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'As we had seen earlier, the Age feature has **177** null values. To replace these NaN values, we can assign them the mean age of the dataset.\\\\n\\', \\'\\\\n\\', \\'But the problem is, there were many people with many different ages. We just cant assign a 4 year kid with the mean age that is 29 years. Is there any way to find out what age-band does the passenger lie??\\\\n\\', \\'\\\\n\\', \\'**Bingo!!!!**, we can check the **Name**  feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Thus we can assign the mean values of Mr and Mrs to the respective groups.\\\\n\\', \\'\\\\n\\', \"**\\'\\'What\\'s In A Name??\\'\\'**---> **Feature**  :p\"]\\'\\n\\n \\'code\\' cell: \\'[\"data[\\'Initial\\']=0\\\\n\", \\'for i in data:\\\\n\\', \"    data[\\'Initial\\']=data.Name.str.extract(\\'([A-Za-z]+)\\\\\\\\.\\') #lets extract the Salutations\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'Okay so here we are using the Regex: **[A-Za-z]+)\\\\\\\\.**. So what it does is, it looks for strings which lie between **A-Z or a-z** and followed by a **.(dot)**. So we successfully extract the Initials from the Name.\\']\\'\\n\\n \\'code\\' cell: \\'[\"pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap=\\'summer_r\\') #Checking the Initials with the Sex\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'Okay so there are some misspelled Initials like Mlle or Mme that stand for Miss. I will replace them with Miss and same thing for other values.\\']\\'\\n\\n \\'code\\' cell: \\'[\"data[\\'Initial\\'].replace([\\'Mlle\\',\\'Mme\\',\\'Ms\\',\\'Dr\\',\\'Major\\',\\'Lady\\',\\'Countess\\',\\'Jonkheer\\',\\'Col\\',\\'Rev\\',\\'Capt\\',\\'Sir\\',\\'Don\\'],[\\'Miss\\',\\'Miss\\',\\'Miss\\',\\'Mr\\',\\'Mr\\',\\'Mrs\\',\\'Mrs\\',\\'Other\\',\\'Other\\',\\'Other\\',\\'Mr\\',\\'Mr\\',\\'Mr\\'],inplace=True)\"]\\'\\n\\n \\'code\\' cell: \\'[\"data.groupby(\\'Initial\\')[\\'Age\\'].mean() #lets check the average age by Initials\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Filling NaN Ages\\']\\'\\n\\n \\'code\\' cell: \\'[\\'## Assigning the NaN Values with the Ceil values of the mean ages\\\\n\\', \"data.loc[(data.Age.isnull())&(data.Initial==\\'Mr\\'),\\'Age\\']=33\\\\n\", \"data.loc[(data.Age.isnull())&(data.Initial==\\'Mrs\\'),\\'Age\\']=36\\\\n\", \"data.loc[(data.Age.isnull())&(data.Initial==\\'Master\\'),\\'Age\\']=5\\\\n\", \"data.loc[(data.Age.isnull())&(data.Initial==\\'Miss\\'),\\'Age\\']=22\\\\n\", \"data.loc[(data.Age.isnull())&(data.Initial==\\'Other\\'),\\'Age\\']=46\"]\\'\\n\\n \\'code\\' cell: \\'[\\'data.Age.isnull().any() #So no null values left finally \\']\\'\\n\\n \\'code\\' cell: \\'[\\'f,ax=plt.subplots(1,2,figsize=(20,10))\\\\n\\', \"data[data[\\'Survived\\']==0].Age.plot.hist(ax=ax[0],bins=20,edgecolor=\\'black\\',color=\\'red\\')\\\\n\", \"ax[0].set_title(\\'Survived= 0\\')\\\\n\", \\'x1=list(range(0,85,5))\\\\n\\', \\'ax[0].set_xticks(x1)\\\\n\\', \"data[data[\\'Survived\\']==1].Age.plot.hist(ax=ax[1],color=\\'green\\',bins=20,edgecolor=\\'black\\')\\\\n\", \"ax[1].set_title(\\'Survived= 1\\')\\\\n\", \\'x2=list(range(0,85,5))\\\\n\\', \\'ax[1].set_xticks(x2)\\\\n\\', \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Observations:\\\\n\\', \\'1)The Toddlers(age<5) were saved in large numbers(The Women and Child First Policy).\\\\n\\', \\'\\\\n\\', \\'2)The oldest Passenger was saved(80 years).\\\\n\\', \\'\\\\n\\', \\'3)Maximum number of deaths were in the age group of 30-40.\\']\\'\\n\\n \\'code\\' cell: \\'[\"sns.factorplot(\\'Pclass\\',\\'Survived\\',col=\\'Initial\\',data=data)\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'The Women and Child first policy thus holds true irrespective of the class.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Embarked--> Categorical Value\\']\\'\\n\\n \\'code\\' cell: \\'[\"pd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=True).style.background_gradient(cmap=\\'summer_r\\')\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Chances for Survival by Port Of Embarkation\\']\\'\\n\\n \\'code\\' cell: \\'[\"sns.factorplot(\\'Embarked\\',\\'Survived\\',data=data)\\\\n\", \\'fig=plt.gcf()\\\\n\\', \\'fig.set_size_inches(5,3)\\\\n\\', \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'The chances for survival for Port C is highest around 0.55 while it is lowest for S.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'f,ax=plt.subplots(2,2,figsize=(20,15))\\\\n\\', \"sns.countplot(\\'Embarked\\',data=data,ax=ax[0,0])\\\\n\", \"ax[0,0].set_title(\\'No. Of Passengers Boarded\\')\\\\n\", \"sns.countplot(\\'Embarked\\',hue=\\'Sex\\',data=data,ax=ax[0,1])\\\\n\", \"ax[0,1].set_title(\\'Male-Female Split for Embarked\\')\\\\n\", \"sns.countplot(\\'Embarked\\',hue=\\'Survived\\',data=data,ax=ax[1,0])\\\\n\", \"ax[1,0].set_title(\\'Embarked vs Survived\\')\\\\n\", \"sns.countplot(\\'Embarked\\',hue=\\'Pclass\\',data=data,ax=ax[1,1])\\\\n\", \"ax[1,1].set_title(\\'Embarked vs Pclass\\')\\\\n\", \\'plt.subplots_adjust(wspace=0.2,hspace=0.5)\\\\n\\', \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Observations:\\\\n\\', \\'1)Maximum passenegers boarded from S. Majority of them being from Pclass3.\\\\n\\', \\'\\\\n\\', \\'2)The Passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass1 and Pclass2 Passengers.\\\\n\\', \\'\\\\n\\', \"3)The Embark S looks to the port from where majority of the rich people boarded. Still the chances for survival is low here, that is because many passengers from Pclass3 around **81%** didn\\'t survive. \\\\n\", \\'\\\\n\\', \\'4)Port Q had almost 95% of the passengers were from Pclass3.\\']\\'\\n\\n \\'code\\' cell: \\'[\"sns.factorplot(\\'Pclass\\',\\'Survived\\',hue=\\'Sex\\',col=\\'Embarked\\',data=data)\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Observations:\\\\n\\', \\'\\\\n\\', \\'1)The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Pclass.\\\\n\\', \\'\\\\n\\', \\'2)Port S looks to be very unlucky for Pclass3 Passenegers as the survival rate for both men and women is very low.**(Money Matters)**\\\\n\\', \\'\\\\n\\', \\'3)Port Q looks looks to be unlukiest for Men, as almost all were from Pclass 3.\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Filling Embarked NaN\\\\n\\', \\'\\\\n\\', \\'As we saw that maximum passengers boarded from Port S, we replace NaN with S.\\']\\'\\n\\n \\'code\\' cell: \\'[\"data[\\'Embarked\\'].fillna(\\'S\\',inplace=True)\"]\\'\\n\\n \\'code\\' cell: \\'[\\'data.Embarked.isnull().any()# Finally No NaN values\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## SibSip-->Discrete Feature\\\\n\\', \\'This feature represents whether a person is alone or with his family members.\\\\n\\', \\'\\\\n\\', \\'Sibling = brother, sister, stepbrother, stepsister\\\\n\\', \\'\\\\n\\', \\'Spouse = husband, wife \\']\\'\\n\\n \\'code\\' cell: \\'[\"pd.crosstab([data.SibSp],data.Survived).style.background_gradient(cmap=\\'summer_r\\')\"]\\'\\n\\n \\'code\\' cell: \\'[\\'f,ax=plt.subplots(1,2,figsize=(20,8))\\\\n\\', \"sns.barplot(\\'SibSp\\',\\'Survived\\',data=data,ax=ax[0])\\\\n\", \"ax[0].set_title(\\'SibSp vs Survived\\')\\\\n\", \"sns.factorplot(\\'SibSp\\',\\'Survived\\',data=data,ax=ax[1])\\\\n\", \"ax[1].set_title(\\'SibSp vs Survived\\')\\\\n\", \\'plt.close(2)\\\\n\\', \\'plt.show()\\']\\'\\n\\n \\'code\\' cell: \\'[\"pd.crosstab(data.SibSp,data.Pclass).style.background_gradient(cmap=\\'summer_r\\')\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Observations:\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'The barplot and factorplot shows that if a passenger is alone onboard with no siblings, he have 34.5% survival rate. The graph roughly decreases if the number of siblings increase. This makes sense. That is, if I have a family on board, I will try to save them instead of saving myself first. Surprisingly the survival for families with 5-8 members is **0%**. The reason may be Pclass??\\\\n\\', \\'\\\\n\\', \\'The reason is **Pclass**. The crosstab shows that Person with SibSp>3 were all in Pclass3. It is imminent that all the large families in Pclass3(>3) died.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Parch\\']\\'\\n\\n \\'code\\' cell: \\'[\"pd.crosstab(data.Parch,data.Pclass).style.background_gradient(cmap=\\'summer_r\\')\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'The crosstab again shows that larger families were in Pclass3.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'f,ax=plt.subplots(1,2,figsize=(20,8))\\\\n\\', \"sns.barplot(\\'Parch\\',\\'Survived\\',data=data,ax=ax[0])\\\\n\", \"ax[0].set_title(\\'Parch vs Survived\\')\\\\n\", \"sns.factorplot(\\'Parch\\',\\'Survived\\',data=data,ax=ax[1])\\\\n\", \"ax[1].set_title(\\'Parch vs Survived\\')\\\\n\", \\'plt.close(2)\\\\n\\', \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Observations:\\\\n\\', \\'\\\\n\\', \\'Here too the results are quite similar. Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up.\\\\n\\', \\'\\\\n\\', \\'The chances of survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when somebody has >4 parents on the ship.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Fare--> Continous Feature\\']\\'\\n\\n \\'code\\' cell: \\'[\"print(\\'Highest Fare was:\\',data[\\'Fare\\'].max())\\\\n\", \"print(\\'Lowest Fare was:\\',data[\\'Fare\\'].min())\\\\n\", \"print(\\'Average Fare was:\\',data[\\'Fare\\'].mean())\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'The lowest fare is **0.0**. Wow!! a free luxorious ride. \\']\\'\\n\\n \\'code\\' cell: \\'[\\'f,ax=plt.subplots(1,3,figsize=(20,8))\\\\n\\', \"sns.distplot(data[data[\\'Pclass\\']==1].Fare,ax=ax[0])\\\\n\", \"ax[0].set_title(\\'Fares in Pclass 1\\')\\\\n\", \"sns.distplot(data[data[\\'Pclass\\']==2].Fare,ax=ax[1])\\\\n\", \"ax[1].set_title(\\'Fares in Pclass 2\\')\\\\n\", \"sns.distplot(data[data[\\'Pclass\\']==3].Fare,ax=ax[2])\\\\n\", \"ax[2].set_title(\\'Fares in Pclass 3\\')\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'There looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the standards reduces. As this is also continous, we can convert into discrete values by using binning.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Observations in a Nutshell for all features:\\\\n\\', \\'**Sex:** The chance of survival for women is high as compared to men.\\\\n\\', \\'\\\\n\\', \\'**Pclass:**There is a visible trend that being a **1st class passenger** gives you better chances of survival. The survival rate for **Pclass3 is very low**. For **women**, the chance of survival from **Pclass1** is almost 1 and is high too for those from **Pclass2**.   **Money Wins!!!**. \\\\n\\', \\'\\\\n\\', \\'**Age:** Children less than 5-10 years do have a high chance of survival. Passengers between age group 15 to 35 died a lot.\\\\n\\', \\'\\\\n\\', \\'**Embarked:** This is a very interesting feature. **The chances of survival at C looks to be better than even though the majority of Pclass1 passengers got up at S.** Passengers at Q were all from **Pclass3**. \\\\n\\', \\'\\\\n\\', \\'**Parch+SibSp:** Having 1-2 siblings,spouse on board or 1-3 Parents shows a greater chance of probablity rather than being alone or having a large family travelling with you.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Correlation Between The Features\\']\\'\\n\\n \\'code\\' cell: \\'[\"sns.heatmap(data.corr(),annot=True,cmap=\\'RdYlGn\\',linewidths=0.2) #data.corr()-->correlation matrix\\\\n\", \\'fig=plt.gcf()\\\\n\\', \\'fig.set_size_inches(10,8)\\\\n\\', \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Interpreting The Heatmap\\\\n\\', \\'\\\\n\\', \\'The first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\\\\n\\', \\'\\\\n\\', \\'**POSITIVE CORRELATION:** If an **increase in feature A leads to increase in feature B, then they are positively correlated**. A value **1 means perfect positive correlation**.\\\\n\\', \\'\\\\n\\', \\'**NEGATIVE CORRELATION:** If an **increase in feature A leads to decrease in feature B, then they are negatively correlated**. A value **-1 means perfect negative correlation**.\\\\n\\', \\'\\\\n\\', \\'Now lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as **MultiColinearity** as both of them contains almost the same information.\\\\n\\', \\'\\\\n\\', \\'So do you think we should use both of them as **one of them is redundant**. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\\\\n\\', \\'\\\\n\\', \\'Now from the above heatmap,we can see that the features are not much correlated. The highest correlation is between **SibSp and Parch i.e 0.41**. So we can carry on with all features.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Part2: Feature Engineering and Data Cleaning\\\\n\\', \\'\\\\n\\', \\'Now what is Feature Engineering?\\\\n\\', \\'\\\\n\\', \\'Whenever we are given a dataset with features, it is not necessary that all the features will be important. There maybe be many redundant features which should be eliminated. Also we can get or add new features by observing or extracting information from other features.\\\\n\\', \\'\\\\n\\', \\'An example would be getting the Initals feature using the Name Feature. Lets see if we can get any new features and eliminate a few. Also we will tranform the existing relevant features to suitable form for Predictive Modeling.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Age_band\\\\n\\', \\'\\\\n\\', \\'#### Problem With Age Feature:\\\\n\\', \\'As I have mentioned earlier that **Age is a continous feature**, there is a problem with Continous Variables in Machine Learning Models.\\\\n\\', \\'\\\\n\\', \\'**Eg:**If I say to group or arrange Sports Person by **Sex**, We can easily segregate them by Male and Female.\\\\n\\', \\'\\\\n\\', \\'Now if I say to group them by their **Age**, then how would you do it? If there are 30 Persons, there may be 30 age values. Now this is problematic.\\\\n\\', \\'\\\\n\\', \\'We need to convert these **continous values into categorical values** by either Binning or Normalisation. I will be using binning i.e group a range of ages into a single bin or assign them a single value.\\\\n\\', \\'\\\\n\\', \\'Okay so the maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins. So 80/5=16.\\\\n\\', \\'So bins of size 16.\\']\\'\\n\\n \\'code\\' cell: \\'[\"data[\\'Age_band\\']=0\\\\n\", \"data.loc[data[\\'Age\\']<=16,\\'Age_band\\']=0\\\\n\", \"data.loc[(data[\\'Age\\']>16)&(data[\\'Age\\']<=32),\\'Age_band\\']=1\\\\n\", \"data.loc[(data[\\'Age\\']>32)&(data[\\'Age\\']<=48),\\'Age_band\\']=2\\\\n\", \"data.loc[(data[\\'Age\\']>48)&(data[\\'Age\\']<=64),\\'Age_band\\']=3\\\\n\", \"data.loc[data[\\'Age\\']>64,\\'Age_band\\']=4\\\\n\", \\'data.head(2)\\']\\'\\n\\n \\'code\\' cell: \\'[\"data[\\'Age_band\\'].value_counts().to_frame().style.background_gradient(cmap=\\'summer\\')#checking the number of passenegers in each band\"]\\'\\n\\n \\'code\\' cell: \\'[\"sns.factorplot(\\'Age_band\\',\\'Survived\\',data=data,col=\\'Pclass\\')\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'True that..the survival rate decreases as the age increases irrespective of the Pclass.\\\\n\\', \\'\\\\n\\', \\'## Family_Size and Alone\\\\n\\', \\'At this point, we can create a new feature called \"Family_size\" and \"Alone\" and analyse it. This feature is the summation of Parch and SibSp. It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. Alone will denote whether a passenger is alone or not.\\']\\'\\n\\n \\'code\\' cell: \\'[\"data[\\'Family_Size\\']=0\\\\n\", \"data[\\'Family_Size\\']=data[\\'Parch\\']+data[\\'SibSp\\']#family size\\\\n\", \"data[\\'Alone\\']=0\\\\n\", \"data.loc[data.Family_Size==0,\\'Alone\\']=1#Alone\\\\n\", \\'\\\\n\\', \\'f,ax=plt.subplots(1,2,figsize=(18,6))\\\\n\\', \"sns.factorplot(\\'Family_Size\\',\\'Survived\\',data=data,ax=ax[0])\\\\n\", \"ax[0].set_title(\\'Family_Size vs Survived\\')\\\\n\", \"sns.factorplot(\\'Alone\\',\\'Survived\\',data=data,ax=ax[1])\\\\n\", \"ax[1].set_title(\\'Alone vs Survived\\')\\\\n\", \\'plt.close(2)\\\\n\\', \\'plt.close(3)\\\\n\\', \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'**Family_Size=0 means that the passeneger is alone.** Clearly, if you are alone or family_size=0,then chances for survival is very low. For family size > 4,the chances decrease too. This also looks to be an important feature for the model. Lets examine this further.\\']\\'\\n\\n \\'code\\' cell: \\'[\"sns.factorplot(\\'Alone\\',\\'Survived\\',data=data,hue=\\'Sex\\',col=\\'Pclass\\')\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'It is visible that being alone is harmful irrespective of Sex or Pclass except for Pclass3 where the chances of females who are alone is high than those with family.\\\\n\\', \\'\\\\n\\', \\'## Fare_Range\\\\n\\', \\'\\\\n\\', \\'Since fare is also a continous feature, we need to convert it into ordinal value. For this we will use **pandas.qcut**.\\\\n\\', \\'\\\\n\\', \\'So what **qcut** does is it splits or arranges the values according the number of bins we have passed. So if we pass for 5 bins, it will arrange the values equally spaced into 5 seperate bins or value ranges.\\']\\'\\n\\n \\'code\\' cell: \\'[\"data[\\'Fare_Range\\']=pd.qcut(data[\\'Fare\\'],4)\\\\n\", \"data.groupby([\\'Fare_Range\\'])[\\'Survived\\'].mean().to_frame().style.background_gradient(cmap=\\'summer_r\\')\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'As discussed above, we can clearly see that as the **fare_range increases, the chances of survival increases.**\\\\n\\', \\'\\\\n\\', \\'Now we cannot pass the Fare_Range values as it is. We should convert it into singleton values same as we did in **Age_Band**\\']\\'\\n\\n \\'code\\' cell: \\'[\"data[\\'Fare_cat\\']=0\\\\n\", \"data.loc[data[\\'Fare\\']<=7.91,\\'Fare_cat\\']=0\\\\n\", \"data.loc[(data[\\'Fare\\']>7.91)&(data[\\'Fare\\']<=14.454),\\'Fare_cat\\']=1\\\\n\", \"data.loc[(data[\\'Fare\\']>14.454)&(data[\\'Fare\\']<=31),\\'Fare_cat\\']=2\\\\n\", \"data.loc[(data[\\'Fare\\']>31)&(data[\\'Fare\\']<=513),\\'Fare_cat\\']=3\"]\\'\\n\\n \\'code\\' cell: \\'[\"sns.factorplot(\\'Fare_cat\\',\\'Survived\\',data=data,hue=\\'Sex\\')\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Clearly, as the Fare_cat increases, the survival chances increases. This feature may become an important feature during modeling along with the Sex.\\\\n\\', \\'\\\\n\\', \\'## Converting String Values into Numeric\\\\n\\', \\'\\\\n\\', \\'Since we cannot pass strings to a machine learning model, we need to convert features loke Sex, Embarked, etc into numeric values.\\']\\'\\n\\n \\'code\\' cell: \\'[\"data[\\'Sex\\'].replace([\\'male\\',\\'female\\'],[0,1],inplace=True)\\\\n\", \"data[\\'Embarked\\'].replace([\\'S\\',\\'C\\',\\'Q\\'],[0,1,2],inplace=True)\\\\n\", \"data[\\'Initial\\'].replace([\\'Mr\\',\\'Mrs\\',\\'Miss\\',\\'Master\\',\\'Other\\'],[0,1,2,3,4],inplace=True)\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Dropping UnNeeded Features\\\\n\\', \\'\\\\n\\', \"**Name**--> We don\\'t need name feature as it cannot be converted into any categorical value.\\\\n\", \\'\\\\n\\', \\'**Age**--> We have the Age_band feature, so no need of this.\\\\n\\', \\'\\\\n\\', \\'**Ticket**--> It is any random string that cannot be categorised.\\\\n\\', \\'\\\\n\\', \\'**Fare**--> We have the Fare_cat feature, so unneeded\\\\n\\', \\'\\\\n\\', \\'**Cabin**--> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\\\\n\\', \\'\\\\n\\', \\'**Fare_Range**--> We have the fare_cat feature.\\\\n\\', \\'\\\\n\\', \\'**PassengerId**--> Cannot be categorised.\\']\\'\\n\\n \\'code\\' cell: \\'[\"data.drop([\\'Name\\',\\'Age\\',\\'Ticket\\',\\'Fare\\',\\'Cabin\\',\\'Fare_Range\\',\\'PassengerId\\'],axis=1,inplace=True)\\\\n\", \"sns.heatmap(data.corr(),annot=True,cmap=\\'RdYlGn\\',linewidths=0.2,annot_kws={\\'size\\':20})\\\\n\", \\'fig=plt.gcf()\\\\n\\', \\'fig.set_size_inches(18,15)\\\\n\\', \\'plt.xticks(fontsize=14)\\\\n\\', \\'plt.yticks(fontsize=14)\\\\n\\', \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Now the above correlation plot, we can see some positively related features. Some of them being **SibSp andd Family_Size** and **Parch and Family_Size** and some negative ones like **Alone and Family_Size.**\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Part3: Predictive Modeling\\\\n\\', \\'\\\\n\\', \\'We have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a passenger will survive or die. So now we will predict the whether the Passenger will survive or not using some great Classification Algorithms.Following are the algorithms I will use to make the model:\\\\n\\', \\'\\\\n\\', \\'1)Logistic Regression\\\\n\\', \\'\\\\n\\', \\'2)Support Vector Machines(Linear and radial)\\\\n\\', \\'\\\\n\\', \\'3)Random Forest\\\\n\\', \\'\\\\n\\', \\'4)K-Nearest Neighbours\\\\n\\', \\'\\\\n\\', \\'5)Naive Bayes\\\\n\\', \\'\\\\n\\', \\'6)Decision Tree\\\\n\\', \\'\\\\n\\', \\'7)Logistic Regression\\']\\'\\n\\n \\'code\\' cell: \\'[\\'#importing all the required ML packages\\\\n\\', \\'from sklearn.linear_model import LogisticRegression #logistic regression\\\\n\\', \\'from sklearn import svm #support vector Machine\\\\n\\', \\'from sklearn.ensemble import RandomForestClassifier #Random Forest\\\\n\\', \\'from sklearn.neighbors import KNeighborsClassifier #KNN\\\\n\\', \\'from sklearn.naive_bayes import GaussianNB #Naive bayes\\\\n\\', \\'from sklearn.tree import DecisionTreeClassifier #Decision Tree\\\\n\\', \\'from sklearn.model_selection import train_test_split #training and testing data split\\\\n\\', \\'from sklearn import metrics #accuracy measure\\\\n\\', \\'from sklearn.metrics import confusion_matrix #for confusion matrix\\']\\'\\n\\n \\'code\\' cell: \\'[\"train,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data[\\'Survived\\'])\\\\n\", \\'train_X=train[train.columns[1:]]\\\\n\\', \\'train_Y=train[train.columns[:1]]\\\\n\\', \\'test_X=test[test.columns[1:]]\\\\n\\', \\'test_Y=test[test.columns[:1]]\\\\n\\', \\'X=data[data.columns[1:]]\\\\n\\', \"Y=data[\\'Survived\\']\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Radial Support Vector Machines(rbf-SVM)\\']\\'\\n\\n \\'code\\' cell: \\'[\"model=svm.SVC(kernel=\\'rbf\\',C=1,gamma=0.1)\\\\n\", \\'model.fit(train_X,train_Y)\\\\n\\', \\'prediction1=model.predict(test_X)\\\\n\\', \"print(\\'Accuracy for rbf SVM is \\',metrics.accuracy_score(prediction1,test_Y))\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Linear Support Vector Machine(linear-SVM)\\']\\'\\n\\n \\'code\\' cell: \\'[\"model=svm.SVC(kernel=\\'linear\\',C=0.1,gamma=0.1)\\\\n\", \\'model.fit(train_X,train_Y)\\\\n\\', \\'prediction2=model.predict(test_X)\\\\n\\', \"print(\\'Accuracy for linear SVM is\\',metrics.accuracy_score(prediction2,test_Y))\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Logistic Regression\\']\\'\\n\\n \\'code\\' cell: \\'[\\'model = LogisticRegression()\\\\n\\', \\'model.fit(train_X,train_Y)\\\\n\\', \\'prediction3=model.predict(test_X)\\\\n\\', \"print(\\'The accuracy of the Logistic Regression is\\',metrics.accuracy_score(prediction3,test_Y))\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Decision Tree\\']\\'\\n\\n \\'code\\' cell: \\'[\\'model=DecisionTreeClassifier()\\\\n\\', \\'model.fit(train_X,train_Y)\\\\n\\', \\'prediction4=model.predict(test_X)\\\\n\\', \"print(\\'The accuracy of the Decision Tree is\\',metrics.accuracy_score(prediction4,test_Y))\\\\n\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'### K-Nearest Neighbours(KNN)\\']\\'\\n\\n \\'code\\' cell: \\'[\\'model=KNeighborsClassifier() \\\\n\\', \\'model.fit(train_X,train_Y)\\\\n\\', \\'prediction5=model.predict(test_X)\\\\n\\', \"print(\\'The accuracy of the KNN is\\',metrics.accuracy_score(prediction5,test_Y))\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'Now the accuracy for the KNN model changes as we change the values for **n_neighbours** attribute. The default value is **5**. Lets check the accuracies over various values of n_neighbours.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'a_index=list(range(1,11))\\\\n\\', \\'a=pd.Series()\\\\n\\', \\'x=[0,1,2,3,4,5,6,7,8,9,10]\\\\n\\', \\'for i in list(range(1,11)):\\\\n\\', \\'    model=KNeighborsClassifier(n_neighbors=i) \\\\n\\', \\'    model.fit(train_X,train_Y)\\\\n\\', \\'    prediction=model.predict(test_X)\\\\n\\', \\'    a=a.append(pd.Series(metrics.accuracy_score(prediction,test_Y)))\\\\n\\', \\'plt.plot(a_index, a)\\\\n\\', \\'plt.xticks(x)\\\\n\\', \\'fig=plt.gcf()\\\\n\\', \\'fig.set_size_inches(12,6)\\\\n\\', \\'plt.show()\\\\n\\', \"print(\\'Accuracies for different values of n are:\\',a.values,\\'with the max value as \\',a.values.max())\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Gaussian Naive Bayes\\']\\'\\n\\n \\'code\\' cell: \\'[\\'model=GaussianNB()\\\\n\\', \\'model.fit(train_X,train_Y)\\\\n\\', \\'prediction6=model.predict(test_X)\\\\n\\', \"print(\\'The accuracy of the NaiveBayes is\\',metrics.accuracy_score(prediction6,test_Y))\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Random Forests\\']\\'\\n\\n \\'code\\' cell: \\'[\\'model=RandomForestClassifier(n_estimators=100)\\\\n\\', \\'model.fit(train_X,train_Y)\\\\n\\', \\'prediction7=model.predict(test_X)\\\\n\\', \"print(\\'The accuracy of the Random Forests is\\',metrics.accuracy_score(prediction7,test_Y))\"]\\'\\n\\n \\'markdown\\' cell: \\'[\"The accuracy of a model is not the only factor that determines the robustness of the classifier. Let\\'s say that a classifier is trained over a training data and tested over the test data and it scores an accuracy of 90%.\\\\n\", \\'\\\\n\\', \"Now this seems to be very good accuracy for a classifier, but can we confirm that it will be 90% for all the new test sets that come over??. The answer is **No**, because we can\\'t determine which all instances will the classifier will use to train itself. As the training and testing data changes, the accuracy will also change. It may increase or decrease. This is known as **model variance**.\\\\n\", \\'\\\\n\\', \\'To overcome this and get a generalized model,we use **Cross Validation**.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'# Cross Validation\\\\n\\', \\'\\\\n\\', \\'Many a times, the data is imbalanced, i.e there may be a high number of class1 instances but less number of other class instances. Thus we should train and test our algorithm on each and every instance of the dataset. Then we can take an average of all the noted accuracies over the dataset. \\\\n\\', \\'\\\\n\\', \\'1)The K-Fold Cross Validation works by first dividing the dataset into k-subsets.\\\\n\\', \\'\\\\n\\', \"2)Let\\'s say we divide the dataset into (k=5) parts. We reserve 1 part for testing and train the algorithm over the 4 parts.\\\\n\", \\'\\\\n\\', \\'3)We continue the process by changing the testing part in each iteration and training the algorithm over the other parts. The accuracies and errors are then averaged to get a average accuracy of the algorithm.\\\\n\\', \\'\\\\n\\', \\'This is called K-Fold Cross Validation.\\\\n\\', \\'\\\\n\\', \\'4)An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'from sklearn.model_selection import KFold #for K-fold cross validation\\\\n\\', \\'from sklearn.model_selection import cross_val_score #score evaluation\\\\n\\', \\'from sklearn.model_selection import cross_val_predict #prediction\\\\n\\', \\'kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\\\\n\\', \\'xyz=[]\\\\n\\', \\'accuracy=[]\\\\n\\', \\'std=[]\\\\n\\', \"classifiers=[\\'Linear Svm\\',\\'Radial Svm\\',\\'Logistic Regression\\',\\'KNN\\',\\'Decision Tree\\',\\'Naive Bayes\\',\\'Random Forest\\']\\\\n\", \"models=[svm.SVC(kernel=\\'linear\\'),svm.SVC(kernel=\\'rbf\\'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\\\\n\", \\'for i in models:\\\\n\\', \\'    model = i\\\\n\\', \\'    cv_result = cross_val_score(model,X,Y, cv = kfold,scoring = \"accuracy\")\\\\n\\', \\'    cv_result=cv_result\\\\n\\', \\'    xyz.append(cv_result.mean())\\\\n\\', \\'    std.append(cv_result.std())\\\\n\\', \\'    accuracy.append(cv_result)\\\\n\\', \"new_models_dataframe2=pd.DataFrame({\\'CV Mean\\':xyz,\\'Std\\':std},index=classifiers)       \\\\n\", \\'new_models_dataframe2\\']\\'\\n\\n \\'code\\' cell: \\'[\\'plt.subplots(figsize=(12,6))\\\\n\\', \\'box=pd.DataFrame(accuracy,index=[classifiers])\\\\n\\', \\'box.T.boxplot()\\']\\'\\n\\n \\'code\\' cell: \\'[\"new_models_dataframe2[\\'CV Mean\\'].plot.barh(width=0.8)\\\\n\", \"plt.title(\\'Average CV Mean Accuracy\\')\\\\n\", \\'fig=plt.gcf()\\\\n\\', \\'fig.set_size_inches(8,5)\\\\n\\', \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'The classification accuracy can be sometimes misleading due to imbalance. We can get a summarized result with the help of confusion matrix, which shows where did the model go wrong, or which class did the model predict wrong.\\\\n\\', \\'\\\\n\\', \\'## Confusion Matrix\\\\n\\', \\'\\\\n\\', \\'It gives the number of correct and incorrect classifications made by the classifier.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'f,ax=plt.subplots(3,3,figsize=(12,10))\\\\n\\', \"y_pred = cross_val_predict(svm.SVC(kernel=\\'rbf\\'),X,Y,cv=10)\\\\n\", \"sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,0],annot=True,fmt=\\'2.0f\\')\\\\n\", \"ax[0,0].set_title(\\'Matrix for rbf-SVM\\')\\\\n\", \"y_pred = cross_val_predict(svm.SVC(kernel=\\'linear\\'),X,Y,cv=10)\\\\n\", \"sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,1],annot=True,fmt=\\'2.0f\\')\\\\n\", \"ax[0,1].set_title(\\'Matrix for Linear-SVM\\')\\\\n\", \\'y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X,Y,cv=10)\\\\n\\', \"sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[0,2],annot=True,fmt=\\'2.0f\\')\\\\n\", \"ax[0,2].set_title(\\'Matrix for KNN\\')\\\\n\", \\'y_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X,Y,cv=10)\\\\n\\', \"sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,0],annot=True,fmt=\\'2.0f\\')\\\\n\", \"ax[1,0].set_title(\\'Matrix for Random-Forests\\')\\\\n\", \\'y_pred = cross_val_predict(LogisticRegression(),X,Y,cv=10)\\\\n\\', \"sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,1],annot=True,fmt=\\'2.0f\\')\\\\n\", \"ax[1,1].set_title(\\'Matrix for Logistic Regression\\')\\\\n\", \\'y_pred = cross_val_predict(DecisionTreeClassifier(),X,Y,cv=10)\\\\n\\', \"sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[1,2],annot=True,fmt=\\'2.0f\\')\\\\n\", \"ax[1,2].set_title(\\'Matrix for Decision Tree\\')\\\\n\", \\'y_pred = cross_val_predict(GaussianNB(),X,Y,cv=10)\\\\n\\', \"sns.heatmap(confusion_matrix(Y,y_pred),ax=ax[2,0],annot=True,fmt=\\'2.0f\\')\\\\n\", \"ax[2,0].set_title(\\'Matrix for Naive Bayes\\')\\\\n\", \\'plt.subplots_adjust(hspace=0.2,wspace=0.2)\\\\n\\', \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Interpreting Confusion Matrix\\\\n\\', \\'\\\\n\\', \\'The left diagonal shows the number of correct predictions made for each class while the right diagonal shows the number of wrong prredictions made. Lets consider the first plot for rbf-SVM:\\\\n\\', \\'\\\\n\\', \\'1)The no. of correct predictions are **491(for dead) + 247(for survived)** with the mean CV accuracy being **(491+247)/891 = 82.8%** which we did get earlier.\\\\n\\', \\'\\\\n\\', \\'2)**Errors**-->  Wrongly Classified 58 dead people as survived and 95 survived as dead. Thus it has made more mistakes by predicting dead as survived.\\\\n\\', \\'\\\\n\\', \\'By looking at all the matrices, we can say that rbf-SVM has a higher chance in correctly predicting dead passengers but NaiveBayes has a higher chance in correctly predicting passengers who survived.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Hyper-Parameters Tuning\\\\n\\', \\'\\\\n\\', \\'The machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change to get a better model. Like the C and gamma in the SVM model and similarly different parameters for different classifiers, are called the hyper-parameters, which we can tune to change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning.\\\\n\\', \\'\\\\n\\', \\'We will tune the hyper-parameters for the 2 best classifiers i.e the SVM and RandomForests.\\\\n\\', \\'\\\\n\\', \\'#### SVM\\']\\'\\n\\n \\'code\\' cell: \\'[\\'from sklearn.model_selection import GridSearchCV\\\\n\\', \\'C=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\\\\n\\', \\'gamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\\\\n\\', \"kernel=[\\'rbf\\',\\'linear\\']\\\\n\", \"hyper={\\'kernel\\':kernel,\\'C\\':C,\\'gamma\\':gamma}\\\\n\", \\'gd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\\\\n\\', \\'gd.fit(X,Y)\\\\n\\', \\'print(gd.best_score_)\\\\n\\', \\'print(gd.best_estimator_)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'#### Random Forests\\']\\'\\n\\n \\'code\\' cell: \\'[\\'n_estimators=range(100,1000,100)\\\\n\\', \"hyper={\\'n_estimators\\':n_estimators}\\\\n\", \\'gd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\\\\n\\', \\'gd.fit(X,Y)\\\\n\\', \\'print(gd.best_score_)\\\\n\\', \\'print(gd.best_estimator_)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'The best score for Rbf-Svm is **82.82% with C=0.05 and gamma=0.1**.\\\\n\\', \\'For RandomForest, score is abt **81.8% with n_estimators=900**.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Ensembling\\\\n\\', \\'\\\\n\\', \\'Ensembling is a good way to increase the accuracy or performance of a model. In simple words, it is the combination of various simple models to create a single powerful model.\\\\n\\', \\'\\\\n\\', \\'Lets say we want to buy a phone and ask many people about it based on various parameters. So then we can make a strong judgement about a single product after analysing all different parameters. This is **Ensembling**, which improves the stability of the model. Ensembling can be done in ways like:\\\\n\\', \\'\\\\n\\', \\'1)Voting Classifier\\\\n\\', \\'\\\\n\\', \\'2)Bagging\\\\n\\', \\'\\\\n\\', \\'3)Boosting.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Voting Classifier\\\\n\\', \\'\\\\n\\', \\'It is the simplest way of combining predictions from many different simple machine learning models. It gives an average prediction result based on the prediction of all the submodels. The submodels or the basemodels are all of diiferent types.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'from sklearn.ensemble import VotingClassifier\\\\n\\', \"ensemble_lin_rbf=VotingClassifier(estimators=[(\\'KNN\\',KNeighborsClassifier(n_neighbors=10)),\\\\n\", \"                                              (\\'RBF\\',svm.SVC(probability=True,kernel=\\'rbf\\',C=0.5,gamma=0.1)),\\\\n\", \"                                              (\\'RFor\\',RandomForestClassifier(n_estimators=500,random_state=0)),\\\\n\", \"                                              (\\'LR\\',LogisticRegression(C=0.05)),\\\\n\", \"                                              (\\'DT\\',DecisionTreeClassifier(random_state=0)),\\\\n\", \"                                              (\\'NB\\',GaussianNB()),\\\\n\", \"                                              (\\'svm\\',svm.SVC(kernel=\\'linear\\',probability=True))\\\\n\", \\'                                             ], \\\\n\\', \"                       voting=\\'soft\\').fit(train_X,train_Y)\\\\n\", \"print(\\'The accuracy for ensembled model is:\\',ensemble_lin_rbf.score(test_X,test_Y))\\\\n\", \\'cross=cross_val_score(ensemble_lin_rbf,X,Y, cv = 10,scoring = \"accuracy\")\\\\n\\', \"print(\\'The cross validated score is\\',cross.mean())\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Bagging\\\\n\\', \\'\\\\n\\', \\'Bagging is a general ensemble method. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averaging,there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers.\\\\n\\', \\'\\\\n\\', \\'#### Bagged KNN\\\\n\\', \\'\\\\n\\', \\'Bagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of **n_neighbours**, as small value of n_neighbours.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'from sklearn.ensemble import BaggingClassifier\\\\n\\', \\'model=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\\\\n\\', \\'model.fit(train_X,train_Y)\\\\n\\', \\'prediction=model.predict(test_X)\\\\n\\', \"print(\\'The accuracy for bagged KNN is:\\',metrics.accuracy_score(prediction,test_Y))\\\\n\", \"result=cross_val_score(model,X,Y,cv=10,scoring=\\'accuracy\\')\\\\n\", \"print(\\'The cross validated score for bagged KNN is:\\',result.mean())\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'#### Bagged DecisionTree\\\\n\\']\\'\\n\\n \\'code\\' cell: \\'[\\'model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\\\\n\\', \\'model.fit(train_X,train_Y)\\\\n\\', \\'prediction=model.predict(test_X)\\\\n\\', \"print(\\'The accuracy for bagged Decision Tree is:\\',metrics.accuracy_score(prediction,test_Y))\\\\n\", \"result=cross_val_score(model,X,Y,cv=10,scoring=\\'accuracy\\')\\\\n\", \"print(\\'The cross validated score for bagged Decision Tree is:\\',result.mean())\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Boosting\\\\n\\', \\'\\\\n\\', \\'Boosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a weak model.Boosting works as follows:\\\\n\\', \\'\\\\n\\', \\'A model is first trained on the complete dataset. Now the model will get some instances right while some wrong. Now in the next iteration, the learner will focus more on the wrongly predicted instances or give more weight to it. Thus it will try to predict the wrong instance correctly. Now this iterative process continous, and new classifers are added to the model until the limit is reached on the accuracy.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'#### AdaBoost(Adaptive Boosting)\\\\n\\', \\'\\\\n\\', \\'The weak learner or estimator in this case is a Decsion Tree.  But we can change the dafault base_estimator to any algorithm of our choice.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'from sklearn.ensemble import AdaBoostClassifier\\\\n\\', \\'ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\\\\n\\', \"result=cross_val_score(ada,X,Y,cv=10,scoring=\\'accuracy\\')\\\\n\", \"print(\\'The cross validated score for AdaBoost is:\\',result.mean())\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'#### Stochastic Gradient Boosting\\\\n\\', \\'\\\\n\\', \\'Here too the weak learner is a Decision Tree.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'from sklearn.ensemble import GradientBoostingClassifier\\\\n\\', \\'grad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\\\\n\\', \"result=cross_val_score(grad,X,Y,cv=10,scoring=\\'accuracy\\')\\\\n\", \"print(\\'The cross validated score for Gradient Boosting is:\\',result.mean())\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'#### XGBoost\\']\\'\\n\\n \\'code\\' cell: \\'[\\'import xgboost as xg\\\\n\\', \\'xgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\\\\n\\', \"result=cross_val_score(xgboost,X,Y,cv=10,scoring=\\'accuracy\\')\\\\n\", \"print(\\'The cross validated score for XGBoost is:\\',result.mean())\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'We got the highest accuracy for AdaBoost. We will try to increase it with Hyper-Parameter Tuning\\\\n\\', \\'\\\\n\\', \\'#### Hyper-Parameter Tuning for AdaBoost\\']\\'\\n\\n \\'code\\' cell: \\'[\\'n_estimators=list(range(100,1100,100))\\\\n\\', \\'learn_rate=[0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\\\\n\\', \"hyper={\\'n_estimators\\':n_estimators,\\'learning_rate\\':learn_rate}\\\\n\", \\'gd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\\\\n\\', \\'gd.fit(X,Y)\\\\n\\', \\'print(gd.best_score_)\\\\n\\', \\'print(gd.best_estimator_)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'The maximum accuracy we can get with AdaBoost is **83.16% with n_estimators=200 and learning_rate=0.05**\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'### Confusion Matrix for the Best Model\\']\\'\\n\\n \\'code\\' cell: \\'[\\'ada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\\\\n\\', \\'result=cross_val_predict(ada,X,Y,cv=10)\\\\n\\', \"sns.heatmap(confusion_matrix(Y,result),cmap=\\'winter\\',annot=True,fmt=\\'2.0f\\')\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'## Feature Importance\\']\\'\\n\\n \\'code\\' cell: \\'[\\'f,ax=plt.subplots(2,2,figsize=(15,12))\\\\n\\', \\'model=RandomForestClassifier(n_estimators=500,random_state=0)\\\\n\\', \\'model.fit(X,Y)\\\\n\\', \\'pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\\\\n\\', \"ax[0,0].set_title(\\'Feature Importance in Random Forests\\')\\\\n\", \\'model=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\\\\n\\', \\'model.fit(X,Y)\\\\n\\', \"pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color=\\'#ddff11\\')\\\\n\", \"ax[0,1].set_title(\\'Feature Importance in AdaBoost\\')\\\\n\", \\'model=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\\\\n\\', \\'model.fit(X,Y)\\\\n\\', \"pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap=\\'RdYlGn_r\\')\\\\n\", \"ax[1,0].set_title(\\'Feature Importance in Gradient Boosting\\')\\\\n\", \\'model=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\\\\n\\', \\'model.fit(X,Y)\\\\n\\', \"pd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color=\\'#FD0F00\\')\\\\n\", \"ax[1,1].set_title(\\'Feature Importance in XgBoost\\')\\\\n\", \\'plt.show()\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'We can see the important features for various classifiers like RandomForests, AdaBoost,etc.\\\\n\\', \\'\\\\n\\', \\'#### Observations:\\\\n\\', \\'\\\\n\\', \\'1)Some of the common important features are Initial,Fare_cat,Pclass,Family_Size.\\\\n\\', \\'\\\\n\\', \"2)The Sex feature doesn\\'t seem to give any importance, which is shocking as we had seen earlier that Sex combined with Pclass was giving a very good differentiating factor. Sex looks to be important only in RandomForests.\\\\n\", \\'\\\\n\\', \\'However, we can see the feature Initial, which is at the top in many classifiers.We had already seen the positive correlation between Sex and Initial, so they both refer to the gender.\\\\n\\', \\'\\\\n\\', \\'3)Similarly the Pclass and Fare_cat refer to the status of the passengers and Family_Size with Alone,Parch and SibSp.\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'I hope all of you did gain some insights to Machine Learning. Some other great notebooks for Machine Learning are:\\\\n\\', \\'1) For R:[Divide and Conquer by Oscar Takeshita](https://www.kaggle.com/pliptor/divide-and-conquer-0-82297/notebook)\\\\n\\', \\'\\\\n\\', \\'2)For Python:[Pytanic by Heads and Tails](https://www.kaggle.com/headsortails/pytanic)\\\\n\\', \\'\\\\n\\', \\'3)For Python:[Introduction to Ensembling/Stacking by Anisotropic](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)\\\\n\\', \\'\\\\n\\', \\'### Thanks a lot for having a look at this notebook. If you found this notebook useful, **Do Upvote**.\\\\n\\']\\'\\n\\n \\'code\\' cell: \\'[]\\'\\n\\n', metadata={'source': 'notebooks/eda-to-prediction-dietanic.ipynb'}),\n",
       " Document(page_content='\\'markdown\\' cell: \\'[\\'Notebook Source: https://www.kaggle.com/code/gusthema/titanic-competition-w-tensorflow-decision-forests \\\\n\\', \\'\\\\n\\', \\'---\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Titanic competition with TensorFlow Decision Forests\\\\n\\', \\'\\\\n\\', \\'This notebook will take you through the steps needed to train a baseline Gradient Boosted Trees Model using TensorFlow Decision Forests and creating a submission on the Titanic competition. \\\\n\\', \\'\\\\n\\', \\'This notebook shows:\\\\n\\', \\'\\\\n\\', \\'1. How to do some basic pre-processing. For example, the passenger names will be tokenized, and ticket names will be splitted in parts.\\\\n\\', \\'1. How to train a Gradient Boosted Trees (GBT) with default parameters\\\\n\\', \\'1. How to train a GBT with improved default parameters\\\\n\\', \\'1. How to tune the parameters of a GBTs\\\\n\\', \\'1. How to train and ensemble many GBTs\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Imports dependencies\\']\\'\\n\\n \\'code\\' cell: \\'[\\'import numpy as np\\\\n\\', \\'import pandas as pd\\\\n\\', \\'import os\\\\n\\', \\'\\\\n\\', \\'import tensorflow as tf\\\\n\\', \\'import tensorflow_decision_forests as tfdf\\\\n\\', \\'\\\\n\\', \\'print(f\"Found TF-DF {tfdf.__version__}\")\\']\\'\\n with output: \\'[\\'Found TF-DF 1.2.0\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Load dataset\\']\\'\\n\\n  \\'markdown\\' cell: \\'[\\'# Prepare dataset\\\\n\\', \\'\\\\n\\', \\'We will apply the following transformations on the dataset.\\\\n\\', \\'\\\\n\\', \\'1. Tokenize the names. For example, \"Braund, Mr. Owen Harris\" will become [\"Braund\", \"Mr.\", \"Owen\", \"Harris\"].\\\\n\\', \\'2. Extract any prefix in the ticket. For example ticket \"STON/O2. 3101282\" will become \"STON/O2.\" and 3101282.\\']\\'\\n\\n  \\'markdown\\' cell: \\'[\\'Let\\\\\\'s keep the list of the input features of the model. Notably, we don\\\\\\'t want to train our model on the \"PassengerId\" and \"Ticket\" features.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'input_features = list(preprocessed_train_df.columns)\\\\n\\', \\'input_features.remove(\"Ticket\")\\\\n\\', \\'input_features.remove(\"PassengerId\")\\\\n\\', \\'input_features.remove(\"Survived\")\\\\n\\', \\'#input_features.remove(\"Ticket_number\")\\\\n\\', \\'\\\\n\\', \\'print(f\"Input features: {input_features}\")\\']\\'\\n with output: \\'[\"Input features: [\\'Pclass\\', \\'Name\\', \\'Sex\\', \\'Age\\', \\'SibSp\\', \\'Parch\\', \\'Fare\\', \\'Cabin\\', \\'Embarked\\', \\'Ticket_number\\', \\'Ticket_item\\']\\\\n\"]\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Convert Pandas dataset to TensorFlow Dataset\\']\\'\\n\\n \\'code\\' cell: \\'[\\'def tokenize_names(features, labels=None):\\\\n\\', \\'    \"\"\"Divite the names into tokens. TF-DF can consume text tokens natively.\"\"\"\\\\n\\', \\'    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\\\\n\\', \\'    return features, labels\\\\n\\', \\'\\\\n\\', \\'train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df,label=\"Survived\").map(tokenize_names)\\\\n\\', \\'serving_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Train model with default parameters\\\\n\\', \\'\\\\n\\', \\'### Train model\\\\n\\', \\'\\\\n\\', \\'First, we are training a GradientBoostedTreesModel model with the default parameters.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'model = tfdf.keras.GradientBoostedTreesModel(\\\\n\\', \\'    verbose=0, # Very few logs\\\\n\\', \\'    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\\\\n\\', \\'    exclude_non_specified_features=True, # Only use the features in \"features\"\\\\n\\', \\'    random_seed=1234,\\\\n\\', \\')\\\\n\\', \\'model.fit(train_ds)\\\\n\\', \\'\\\\n\\', \\'self_evaluation = model.make_inspector().evaluation()\\\\n\\', \\'print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")\\']\\'\\n with output: \\'[\\'[INFO 2023-05-18T10:31:05.469776904+00:00 kernel.cc:1214] Loading model from path /tmp/tmpxl2c60xw/model/ with prefix f38ff16f536e4497\\\\n\\', \\'[INFO 2023-05-18T10:31:05.47954519+00:00 abstract_model.cc:1311] Engine \"GradientBoostedTreesQuickScorerExtended\" built\\\\n\\', \\'[INFO 2023-05-18T10:31:05.479865457+00:00 kernel.cc:1046] Use fast generic engine\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Train model with improved default parameters\\\\n\\', \\'\\\\n\\', \"Now you\\'ll use some specific parameters when creating the GBT model\"]\\'\\n\\n \\'code\\' cell: \\'[\\'model = tfdf.keras.GradientBoostedTreesModel(\\\\n\\', \\'    verbose=0, # Very few logs\\\\n\\', \\'    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\\\\n\\', \\'    exclude_non_specified_features=True, # Only use the features in \"features\"\\\\n\\', \\'    \\\\n\\', \\'    #num_trees=2000,\\\\n\\', \\'    \\\\n\\', \\'    # Only for GBT.\\\\n\\', \\'    # A bit slower, but great to understand the model.\\\\n\\', \\'    # compute_permutation_variable_importance=True,\\\\n\\', \\'    \\\\n\\', \\'    # Change the default hyper-parameters\\\\n\\', \\'    # hyperparameter_template=\"benchmark_rank1@v1\",\\\\n\\', \\'    \\\\n\\', \\'    #num_trees=1000,\\\\n\\', \\'    #tuner=tuner\\\\n\\', \\'    \\\\n\\', \\'    min_examples=1,\\\\n\\', \\'    categorical_algorithm=\"RANDOM\",\\\\n\\', \\'    #max_depth=4,\\\\n\\', \\'    shrinkage=0.05,\\\\n\\', \\'    #num_candidate_attributes_ratio=0.2,\\\\n\\', \\'    split_axis=\"SPARSE_OBLIQUE\",\\\\n\\', \\'    sparse_oblique_normalization=\"MIN_MAX\",\\\\n\\', \\'    sparse_oblique_num_projections_exponent=2.0,\\\\n\\', \\'    num_trees=2000,\\\\n\\', \\'    #validation_ratio=0.0,\\\\n\\', \\'    random_seed=1234,\\\\n\\', \\'    \\\\n\\', \\')\\\\n\\', \\'model.fit(train_ds)\\\\n\\', \\'\\\\n\\', \\'self_evaluation = model.make_inspector().evaluation()\\\\n\\', \\'print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")\\']\\'\\n with output: \\'[\\'[INFO 2023-05-18T10:31:10.217810247+00:00 kernel.cc:1214] Loading model from path /tmp/tmp73d7qv4h/model/ with prefix ce08288098554ec5\\\\n\\', \\'[INFO 2023-05-18T10:31:10.227982178+00:00 decision_forest.cc:661] Model loaded with 33 root(s), 1823 node(s), and 10 input feature(s).\\\\n\\', \\'[INFO 2023-05-18T10:31:10.228265252+00:00 kernel.cc:1046] Use fast generic engine\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\"Let\\'s look at the model and you can also notice the information about variable importance that the model figured out\"]\\'\\n\\n \\'code\\' cell: \\'[\\'model.summary()\\']\\'\\n with output: \\'[\\'Model: \"gradient_boosted_trees_model_1\"\\\\n\\', \\'_________________________________________________________________\\\\n\\', \\' Layer (type)                Output Shape              Param #   \\\\n\\', \\'=================================================================\\\\n\\', \\'=================================================================\\\\n\\', \\'Total params: 1\\\\n\\', \\'Trainable params: 0\\\\n\\', \\'Non-trainable params: 1\\\\n\\', \\'_________________________________________________________________\\\\n\\', \\'Type: \"GRADIENT_BOOSTED_TREES\"\\\\n\\', \\'Task: CLASSIFICATION\\\\n\\', \\'Label: \"__LABEL\"\\\\n\\', \\'\\\\n\\', \\'Input Features (11):\\\\n\\', \\'\\\\tAge\\\\n\\', \\'\\\\tCabin\\\\n\\', \\'\\\\tEmbarked\\\\n\\', \\'\\\\tFare\\\\n\\', \\'\\\\tName\\\\n\\', \\'\\\\tParch\\\\n\\', \\'\\\\tPclass\\\\n\\', \\'\\\\tSex\\\\n\\', \\'\\\\tSibSp\\\\n\\', \\'\\\\tTicket_item\\\\n\\', \\'\\\\tTicket_number\\\\n\\', \\'\\\\n\\', \\'No weights\\\\n\\', \\'\\\\n\\', \\'Variable Importance: INV_MEAN_MIN_DEPTH:\\\\n\\', \\'    1.           \"Sex\"  0.576632 ################\\\\n\\', \\'    2.           \"Age\"  0.364297 #######\\\\n\\', \\'    3.          \"Fare\"  0.278839 ####\\\\n\\', \\'    4.          \"Name\"  0.208548 #\\\\n\\', \\'    5. \"Ticket_number\"  0.180792 \\\\n\\', \\'    6.        \"Pclass\"  0.176962 \\\\n\\', \\'    7.         \"Parch\"  0.176659 \\\\n\\', \\'    8.   \"Ticket_item\"  0.175540 \\\\n\\', \\'    9.      \"Embarked\"  0.172339 \\\\n\\', \\'   10.         \"SibSp\"  0.170442 \\\\n\\', \\'\\\\n\\', \\'Variable Importance: NUM_AS_ROOT:\\\\n\\', \\'    1.  \"Sex\" 28.000000 ################\\\\n\\', \\'    2. \"Name\"  5.000000 \\\\n\\', \\'\\\\n\\', \\'Variable Importance: NUM_NODES:\\\\n\\', \\'    1.           \"Age\" 406.000000 ################\\\\n\\', \\'    2.          \"Fare\" 290.000000 ###########\\\\n\\', \\'    3.          \"Name\" 44.000000 #\\\\n\\', \\'    4.   \"Ticket_item\" 42.000000 #\\\\n\\', \\'    5.           \"Sex\" 31.000000 #\\\\n\\', \\'    6.         \"Parch\" 28.000000 \\\\n\\', \\'    7. \"Ticket_number\" 22.000000 \\\\n\\', \\'    8.        \"Pclass\" 15.000000 \\\\n\\', \\'    9.      \"Embarked\" 12.000000 \\\\n\\', \\'   10.         \"SibSp\"  5.000000 \\\\n\\', \\'\\\\n\\', \\'Variable Importance: SUM_SCORE:\\\\n\\', \\'    1.           \"Sex\" 460.497828 ################\\\\n\\', \\'    2.           \"Age\" 355.963333 ############\\\\n\\', \\'    3.          \"Fare\" 292.870316 ##########\\\\n\\', \\'    4.          \"Name\" 108.548952 ###\\\\n\\', \\'    5.        \"Pclass\" 28.132254 \\\\n\\', \\'    6.   \"Ticket_item\" 23.818676 \\\\n\\', \\'    7. \"Ticket_number\" 23.772288 \\\\n\\', \\'    8.         \"Parch\" 19.303155 \\\\n\\', \\'    9.      \"Embarked\"  8.155722 \\\\n\\', \\'   10.         \"SibSp\"  0.015225 \\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'Loss: BINOMIAL_LOG_LIKELIHOOD\\\\n\\', \\'Validation loss value: 1.01542\\\\n\\', \\'Number of trees per iteration: 1\\\\n\\', \\'Node format: NOT_SET\\\\n\\', \\'Number of trees: 33\\\\n\\', \\'Total number of nodes: 1823\\\\n\\', \\'\\\\n\\', \\'Number of nodes by tree:\\\\n\\', \\'Count: 33 Average: 55.2424 StdDev: 5.13473\\\\n\\', \\'Min: 39 Max: 63 Ignored: 0\\\\n\\', \\'----------------------------------------------\\\\n\\', \\'[ 39, 40) 1   3.03%   3.03% #\\\\n\\', \\'[ 40, 41) 0   0.00%   3.03%\\\\n\\', \\'[ 41, 42) 0   0.00%   3.03%\\\\n\\', \\'[ 42, 44) 0   0.00%   3.03%\\\\n\\', \\'[ 44, 45) 0   0.00%   3.03%\\\\n\\', \\'[ 45, 46) 0   0.00%   3.03%\\\\n\\', \\'[ 46, 47) 0   0.00%   3.03%\\\\n\\', \\'[ 47, 49) 2   6.06%   9.09% ###\\\\n\\', \\'[ 49, 50) 2   6.06%  15.15% ###\\\\n\\', \\'[ 50, 51) 0   0.00%  15.15%\\\\n\\', \\'[ 51, 52) 2   6.06%  21.21% ###\\\\n\\', \\'[ 52, 54) 5  15.15%  36.36% #######\\\\n\\', \\'[ 54, 55) 0   0.00%  36.36%\\\\n\\', \\'[ 55, 56) 5  15.15%  51.52% #######\\\\n\\', \\'[ 56, 57) 0   0.00%  51.52%\\\\n\\', \\'[ 57, 59) 4  12.12%  63.64% ######\\\\n\\', \\'[ 59, 60) 7  21.21%  84.85% ##########\\\\n\\', \\'[ 60, 61) 0   0.00%  84.85%\\\\n\\', \\'[ 61, 62) 3   9.09%  93.94% ####\\\\n\\', \\'[ 62, 63] 2   6.06% 100.00% ###\\\\n\\', \\'\\\\n\\', \\'Depth by leafs:\\\\n\\', \\'Count: 928 Average: 4.8847 StdDev: 0.380934\\\\n\\', \\'Min: 2 Max: 5 Ignored: 0\\\\n\\', \\'----------------------------------------------\\\\n\\', \\'[ 2, 3)   1   0.11%   0.11%\\\\n\\', \\'[ 3, 4)  17   1.83%   1.94%\\\\n\\', \\'[ 4, 5)  70   7.54%   9.48% #\\\\n\\', \\'[ 5, 5] 840  90.52% 100.00% ##########\\\\n\\', \\'\\\\n\\', \\'Number of training obs by leaf:\\\\n\\', \\'Count: 928 Average: 28.4127 StdDev: 70.8313\\\\n\\', \\'Min: 1 Max: 438 Ignored: 0\\\\n\\', \\'----------------------------------------------\\\\n\\', \\'[   1,  22) 731  78.77%  78.77% ##########\\\\n\\', \\'[  22,  44)  74   7.97%  86.75% #\\\\n\\', \\'[  44,  66)  37   3.99%  90.73% #\\\\n\\', \\'[  66,  88)   3   0.32%  91.06%\\\\n\\', \\'[  88, 110)   9   0.97%  92.03%\\\\n\\', \\'[ 110, 132)   8   0.86%  92.89%\\\\n\\', \\'[ 132, 154)  18   1.94%  94.83%\\\\n\\', \\'[ 154, 176)   8   0.86%  95.69%\\\\n\\', \\'[ 176, 198)   6   0.65%  96.34%\\\\n\\', \\'[ 198, 220)   2   0.22%  96.55%\\\\n\\', \\'[ 220, 241)   2   0.22%  96.77%\\\\n\\', \\'[ 241, 263)   1   0.11%  96.88%\\\\n\\', \\'[ 263, 285)   2   0.22%  97.09%\\\\n\\', \\'[ 285, 307)   5   0.54%  97.63%\\\\n\\', \\'[ 307, 329)   1   0.11%  97.74%\\\\n\\', \\'[ 329, 351)   2   0.22%  97.95%\\\\n\\', \\'[ 351, 373)   6   0.65%  98.60%\\\\n\\', \\'[ 373, 395)   6   0.65%  99.25%\\\\n\\', \\'[ 395, 417)   2   0.22%  99.46%\\\\n\\', \\'[ 417, 438]   5   0.54% 100.00%\\\\n\\', \\'\\\\n\\', \\'Attribute in nodes:\\\\n\\', \\'\\\\t406 : Age [NUMERICAL]\\\\n\\', \\'\\\\t290 : Fare [NUMERICAL]\\\\n\\', \\'\\\\t44 : Name [CATEGORICAL_SET]\\\\n\\', \\'\\\\t42 : Ticket_item [CATEGORICAL]\\\\n\\', \\'\\\\t31 : Sex [CATEGORICAL]\\\\n\\', \\'\\\\t28 : Parch [NUMERICAL]\\\\n\\', \\'\\\\t22 : Ticket_number [CATEGORICAL]\\\\n\\', \\'\\\\t15 : Pclass [NUMERICAL]\\\\n\\', \\'\\\\t12 : Embarked [CATEGORICAL]\\\\n\\', \\'\\\\t5 : SibSp [NUMERICAL]\\\\n\\', \\'\\\\n\\', \\'Attribute in nodes with depth <= 0:\\\\n\\', \\'\\\\t28 : Sex [CATEGORICAL]\\\\n\\', \\'\\\\t5 : Name [CATEGORICAL_SET]\\\\n\\', \\'\\\\n\\', \\'Attribute in nodes with depth <= 1:\\\\n\\', \\'\\\\t39 : Age [NUMERICAL]\\\\n\\', \\'\\\\t28 : Sex [CATEGORICAL]\\\\n\\', \\'\\\\t21 : Fare [NUMERICAL]\\\\n\\', \\'\\\\t5 : Name [CATEGORICAL_SET]\\\\n\\', \\'\\\\t3 : Pclass [NUMERICAL]\\\\n\\', \\'\\\\t2 : Ticket_number [CATEGORICAL]\\\\n\\', \\'\\\\t1 : Parch [NUMERICAL]\\\\n\\', \\'\\\\n\\', \\'Attribute in nodes with depth <= 2:\\\\n\\', \\'\\\\t102 : Age [NUMERICAL]\\\\n\\', \\'\\\\t65 : Fare [NUMERICAL]\\\\n\\', \\'\\\\t28 : Sex [CATEGORICAL]\\\\n\\', \\'\\\\t15 : Name [CATEGORICAL_SET]\\\\n\\', \\'\\\\t7 : Ticket_number [CATEGORICAL]\\\\n\\', \\'\\\\t5 : Pclass [NUMERICAL]\\\\n\\', \\'\\\\t4 : Parch [NUMERICAL]\\\\n\\', \\'\\\\t2 : Ticket_item [CATEGORICAL]\\\\n\\', \\'\\\\t2 : Embarked [CATEGORICAL]\\\\n\\', \\'\\\\n\\', \\'Attribute in nodes with depth <= 3:\\\\n\\', \\'\\\\t206 : Age [NUMERICAL]\\\\n\\', \\'\\\\t156 : Fare [NUMERICAL]\\\\n\\', \\'\\\\t33 : Name [CATEGORICAL_SET]\\\\n\\', \\'\\\\t29 : Sex [CATEGORICAL]\\\\n\\', \\'\\\\t19 : Ticket_number [CATEGORICAL]\\\\n\\', \\'\\\\t11 : Ticket_item [CATEGORICAL]\\\\n\\', \\'\\\\t11 : Parch [NUMERICAL]\\\\n\\', \\'\\\\t7 : Pclass [NUMERICAL]\\\\n\\', \\'\\\\t3 : Embarked [CATEGORICAL]\\\\n\\', \\'\\\\n\\', \\'Attribute in nodes with depth <= 5:\\\\n\\', \\'\\\\t406 : Age [NUMERICAL]\\\\n\\', \\'\\\\t290 : Fare [NUMERICAL]\\\\n\\', \\'\\\\t44 : Name [CATEGORICAL_SET]\\\\n\\', \\'\\\\t42 : Ticket_item [CATEGORICAL]\\\\n\\', \\'\\\\t31 : Sex [CATEGORICAL]\\\\n\\', \\'\\\\t28 : Parch [NUMERICAL]\\\\n\\', \\'\\\\t22 : Ticket_number [CATEGORICAL]\\\\n\\', \\'\\\\t15 : Pclass [NUMERICAL]\\\\n\\', \\'\\\\t12 : Embarked [CATEGORICAL]\\\\n\\', \\'\\\\t5 : SibSp [NUMERICAL]\\\\n\\', \\'\\\\n\\', \\'Condition type in nodes:\\\\n\\', \\'\\\\t744 : ObliqueCondition\\\\n\\', \\'\\\\t122 : ContainsBitmapCondition\\\\n\\', \\'\\\\t29 : ContainsCondition\\\\n\\', \\'Condition type in nodes with depth <= 0:\\\\n\\', \\'\\\\t31 : ContainsBitmapCondition\\\\n\\', \\'\\\\t2 : ContainsCondition\\\\n\\', \\'Condition type in nodes with depth <= 1:\\\\n\\', \\'\\\\t64 : ObliqueCondition\\\\n\\', \\'\\\\t33 : ContainsBitmapCondition\\\\n\\', \\'\\\\t2 : ContainsCondition\\\\n\\', \\'Condition type in nodes with depth <= 2:\\\\n\\', \\'\\\\t176 : ObliqueCondition\\\\n\\', \\'\\\\t51 : ContainsBitmapCondition\\\\n\\', \\'\\\\t3 : ContainsCondition\\\\n\\', \\'Condition type in nodes with depth <= 3:\\\\n\\', \\'\\\\t380 : ObliqueCondition\\\\n\\', \\'\\\\t77 : ContainsBitmapCondition\\\\n\\', \\'\\\\t18 : ContainsCondition\\\\n\\', \\'Condition type in nodes with depth <= 5:\\\\n\\', \\'\\\\t744 : ObliqueCondition\\\\n\\', \\'\\\\t122 : ContainsBitmapCondition\\\\n\\', \\'\\\\t29 : ContainsCondition\\\\n\\', \\'\\\\n\\', \\'Training logs:\\\\n\\', \\'Number of iteration to final model: 33\\\\n\\', \\'\\\\tIter:1 train-loss:1.266350 valid-loss:1.360049  train-accuracy:0.624531 valid-accuracy:0.543478\\\\n\\', \\'\\\\tIter:2 train-loss:1.213702 valid-loss:1.321897  train-accuracy:0.624531 valid-accuracy:0.543478\\\\n\\', \\'\\\\tIter:3 train-loss:1.165783 valid-loss:1.286817  train-accuracy:0.624531 valid-accuracy:0.543478\\\\n\\', \\'\\\\tIter:4 train-loss:1.122469 valid-loss:1.256133  train-accuracy:0.624531 valid-accuracy:0.543478\\\\n\\', \\'\\\\tIter:5 train-loss:1.081461 valid-loss:1.229342  train-accuracy:0.808511 valid-accuracy:0.771739\\\\n\\', \\'\\\\tIter:6 train-loss:1.045305 valid-loss:1.204601  train-accuracy:0.826033 valid-accuracy:0.728261\\\\n\\', \\'\\\\tIter:16 train-loss:0.794952 valid-loss:1.058568  train-accuracy:0.914894 valid-accuracy:0.771739\\\\n\\', \\'\\\\tIter:26 train-loss:0.646146 valid-loss:1.021539  train-accuracy:0.926158 valid-accuracy:0.793478\\\\n\\', \\'\\\\tIter:36 train-loss:0.558627 valid-loss:1.023663  train-accuracy:0.929912 valid-accuracy:0.771739\\\\n\\', \\'\\\\tIter:46 train-loss:0.493899 valid-loss:1.025164  train-accuracy:0.931164 valid-accuracy:0.760870\\\\n\\', \\'\\\\tIter:56 train-loss:0.451528 valid-loss:1.032880  train-accuracy:0.938673 valid-accuracy:0.771739\\\\n\\', \\'\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Make predictions\\']\\'\\n\\n \\'code\\' cell: \\'[\\'def prediction_to_kaggle_format(model, threshold=0.5):\\\\n\\', \\'    proba_survive = model.predict(serving_ds, verbose=0)[:,0]\\\\n\\', \\'    return pd.DataFrame({\\\\n\\', \\'        \"PassengerId\": serving_df[\"PassengerId\"],\\\\n\\', \\'        \"Survived\": (proba_survive >= threshold).astype(int)\\\\n\\', \\'    })\\\\n\\', \\'\\\\n\\', \\'def make_submission(kaggle_predictions):\\\\n\\', \\'    path=\"/kaggle/working/submission.csv\"\\\\n\\', \\'    kaggle_predictions.to_csv(path, index=False)\\\\n\\', \\'    print(f\"Submission exported to {path}\")\\\\n\\', \\'    \\\\n\\', \\'kaggle_predictions = prediction_to_kaggle_format(model)\\\\n\\', \\'make_submission(kaggle_predictions)\\\\n\\', \\'!head /kaggle/working/submission.csv\\']\\'\\n with output: \\'[\\'Submission exported to /kaggle/working/submission.csv\\\\n\\', \\'PassengerId,Survived\\\\r\\\\n\\', \\'892,0\\\\r\\\\n\\', \\'893,0\\\\r\\\\n\\', \\'894,0\\\\r\\\\n\\', \\'895,0\\\\r\\\\n\\', \\'896,0\\\\r\\\\n\\', \\'897,0\\\\r\\\\n\\', \\'898,0\\\\r\\\\n\\', \\'899,0\\\\r\\\\n\\', \\'900,1\\\\r\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Training a model with hyperparameter tunning\\\\n\\', \\'\\\\n\\', \\'Hyper-parameter tuning is enabled by specifying the tuner constructor argument of the model. The tuner object contains all the configuration of the tuner (search space, optimizer, trial and objective).\\\\n\\']\\'\\n\\n \\'code\\' cell: \\'[\\'tuner = tfdf.tuner.RandomSearch(num_trials=1000)\\\\n\\', \\'tuner.choice(\"min_examples\", [2, 5, 7, 10])\\\\n\\', \\'tuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\\\\n\\', \\'\\\\n\\', \\'local_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\\\\n\\', \\'local_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\\\\n\\', \\'\\\\n\\', \\'global_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\\\\n\\', \\'global_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\\\\n\\', \\'\\\\n\\', \\'#tuner.choice(\"use_hessian_gain\", [True, False])\\\\n\\', \\'tuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\\\\n\\', \\'tuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'tuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\\\\n\\', \\'oblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\\\\n\\', \\'oblique_space.choice(\"sparse_oblique_normalization\",\\\\n\\', \\'                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\\\\n\\', \\'oblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\\\\n\\', \\'oblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])\\\\n\\', \\'\\\\n\\', \\'# Tune the model. Notice the `tuner=tuner`.\\\\n\\', \\'tuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\\\\n\\', \\'tuned_model.fit(train_ds, verbose=0)\\\\n\\', \\'\\\\n\\', \\'tuned_self_evaluation = tuned_model.make_inspector().evaluation()\\\\n\\', \\'print(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss:{tuned_self_evaluation.loss}\")\\']\\'\\n with output: \\'[\\'Use /tmp/tmpf3gqf8yh as temporary training directory\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'In the last line in the cell above, you can see the accuracy is higher than previously with default parameters and parameters set by hand.\\\\n\\', \\'\\\\n\\', \\'This is the main idea behing hyperparameter tuning.\\\\n\\', \\'\\\\n\\', \\'For more information you can follow this tutorial: [Automated hyper-parameter tuning](https://www.tensorflow.org/decision_forests/tutorials/automatic_tuning_colab)\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# Making an ensemble\\\\n\\', \\'\\\\n\\', \"Here you\\'ll create 100 models with different seeds and combine their results\\\\n\", \\'\\\\n\\', \\'This approach removes a little bit the random aspects related to creating ML models\\\\n\\', \\'\\\\n\\', \\'In the GBT creation is used the `honest` parameter. It will use different training examples to infer the structure and the leaf values. This regularization technique trades examples for bias estimates.\\']\\'\\n\\n \\'code\\' cell: \\'[\\'predictions = None\\\\n\\', \\'num_predictions = 0\\\\n\\', \\'\\\\n\\', \\'for i in range(100):\\\\n\\', \\'    print(f\"i:{i}\")\\\\n\\', \\'    # Possible models: GradientBoostedTreesModel or RandomForestModel\\\\n\\', \\'    model = tfdf.keras.GradientBoostedTreesModel(\\\\n\\', \\'        verbose=0, # Very few logs\\\\n\\', \\'        features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\\\\n\\', \\'        exclude_non_specified_features=True, # Only use the features in \"features\"\\\\n\\', \\'\\\\n\\', \\'        #min_examples=1,\\\\n\\', \\'        #categorical_algorithm=\"RANDOM\",\\\\n\\', \\'        ##max_depth=4,\\\\n\\', \\'        #shrinkage=0.05,\\\\n\\', \\'        ##num_candidate_attributes_ratio=0.2,\\\\n\\', \\'        #split_axis=\"SPARSE_OBLIQUE\",\\\\n\\', \\'        #sparse_oblique_normalization=\"MIN_MAX\",\\\\n\\', \\'        #sparse_oblique_num_projections_exponent=2.0,\\\\n\\', \\'        #num_trees=2000,\\\\n\\', \\'        ##validation_ratio=0.0,\\\\n\\', \\'        random_seed=i,\\\\n\\', \\'        honest=True,\\\\n\\', \\'    )\\\\n\\', \\'    model.fit(train_ds)\\\\n\\', \\'    \\\\n\\', \\'    sub_predictions = model.predict(serving_ds, verbose=0)[:,0]\\\\n\\', \\'    if predictions is None:\\\\n\\', \\'        predictions = sub_predictions\\\\n\\', \\'    else:\\\\n\\', \\'        predictions += sub_predictions\\\\n\\', \\'    num_predictions += 1\\\\n\\', \\'\\\\n\\', \\'predictions/=num_predictions\\\\n\\', \\'\\\\n\\', \\'kaggle_predictions = pd.DataFrame({\\\\n\\', \\'        \"PassengerId\": serving_df[\"PassengerId\"],\\\\n\\', \\'        \"Survived\": (predictions >= 0.5).astype(int)\\\\n\\', \\'    })\\\\n\\', \\'\\\\n\\', \\'make_submission(kaggle_predictions)\\']\\'\\n with output: \\'[\\'i:0\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'# What is next\\\\n\\', \\'\\\\n\\', \\'If you want to learn more about TensorFlow Decision Forests and its advanced features, you can follow the official documentation [here](https://www.tensorflow.org/decision_forests) \\']\\'\\n\\n', metadata={'source': 'notebooks/titanic-competition-w-tensorflow-decision-forests.ipynb'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = NotebookLoader(\n",
    "#     path='./notebooks', \n",
    "#     include_outputs=True,\n",
    "#     max_output_length=500,\n",
    "#     traceback=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50, add_start_index=True\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikovalyshyn/pp/talk-to-your-jupyter-notebooks/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# bge-large-en models is approximatelly 1.3G\n",
    "model_name = \"BAAI/bge-large-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "bge_embedding = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=bge_embedding)\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\'markdown\\' cell: \\'[\\'The code above calculates the percentage of male passengers (in **train.csv**) who survived.\\\\n\\', \\'\\\\n\\', \\'From this you can see that almost 75% of the women on board survived, whereas only 19% of the men lived to tell about it. Since gender seems to be such a strong indicator of survival, the submission file in **gender_submission.csv** is not a bad first guess!\\\\n\\', \\'\\\\n\\', \"But at the end of the day, this gender-based submission bases its predictions on only a single column.', metadata={'source': 'notebooks/titanic-tutorial.ipynb', 'start_index': 11500}),\n",
       " Document(page_content='\\'code\\' cell: \\'[\\'women = train_data.loc[train_data.Sex == \\\\\\'female\\\\\\'][\"Survived\"]\\\\n\\', \\'rate_women = sum(women)/len(women)\\\\n\\', \\'\\\\n\\', \\'print(\"% of women who survived:\", rate_women)\\']\\'\\n with output: \\'[\\'% of women who survived: 0.7420382165605095\\\\n\\']\\'\\n\\n \\'markdown\\' cell: \\'[\\'Before moving on, make sure that your code returns the output above.  The code above calculates the percentage of female passengers (in **train.csv**) who survived.\\\\n\\', \\'\\\\n\\', \\'Then, run the code below in another code cell:\\']\\'', metadata={'source': 'notebooks/titanic-tutorial.ipynb', 'start_index': 10769}),\n",
       " Document(page_content=\"'markdown' cell: '['We use **FactorPlot** in this case, because they make the seperation of categorical values easy.\\\\n', '\\\\n', 'Looking at the **CrossTab** and the **FactorPlot**, we can easily infer that survival for **Women from Pclass1** is about **95-96%**, as only 3 out of 94 Women from Pclass1 died. \\\\n', '\\\\n', 'It is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate.\\\\n', '\\\\n', 'Looks like Pclass is also\", metadata={'source': 'notebooks/eda-to-prediction-dietanic.ipynb', 'start_index': 7083}),\n",
       " Document(page_content=\"'markdown' cell: '['## Observations in a Nutshell for all features:\\\\n', '**Sex:** The chance of survival for women is high as compared to men.\\\\n', '\\\\n', '**Pclass:**There is a visible trend that being a **1st class passenger** gives you better chances of survival. The survival rate for **Pclass3 is very low**. For **women**, the chance of survival from **Pclass1** is almost 1 and is high too for those from **Pclass2**.   **Money Wins!!!**. \\\\n', '\\\\n', '**Age:** Children less than 5-10 years do\", metadata={'source': 'notebooks/eda-to-prediction-dietanic.ipynb', 'start_index': 17505})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is the percentage of women who survived?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Do know provide answer if not enough context or you don't know.\n",
    "Answer should provide a reference to the source document file name.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = Ollama(model=\"llama2\")\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the answer to the question \"What is the percentage of women who survived?\" is:\\n\\nAccording to the source document \"notebooks/eda-to-prediction-dietanic.ipynb\" at line 7083, the percentage of women who survived is:\\n\\n\"% of women who survived: 0.95\"\\n\\nReference: notebooks/eda-to-prediction-dietanic.ipynb (start index 7083)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"What is the percentage of women who survived?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the answer to the question \"What is the percentage of men who survived?\" is:\\n\\n\"From the **FactorPlot** and **CrossTab**, we can see that out of 891 passengers in the training set, only around 19% of the men survived the accident. This information can be found in Document(page_content=\\'\\\\\\'markdown\\\\\\' cell: \\\\\\'[It is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass1 have a very low survival rate. \\\\\\\\n\\\\\\', metadata={\\'source\\': \\'notebooks/eda-to-prediction-dietanic.ipynb\\', \\'start_index\\': 7083\\')\"]\\n\\nThe answer is referenced to the source document file name: \"notebooks/eda-to-prediction-dietanic.ipynb\".'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    \"What is the percentage of men who survived?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it seems that the following models have been tried training:\n",
      "\n",
      "1. Gradient Boosted Trees (GBT) with default parameters\n",
      "2. GBT with improved default parameters\n",
      "3. Training a model with hyperparameter tuning\n",
      "\n",
      "These references can be found in the following documents:\n",
      "\n",
      "* Document 1: `notebooks/titanic-competition-w-tensorflow-decision-forests.ipynb`, `start_index`: 17224\n",
      "* Document 2: `notebooks/titanic-competition-w-tensorflow-decision-forests.ipynb`, `start_index`: 591\n",
      "* Document 3: `notebooks/titanic-tutorial.ipynb`, `start_index`: 12403\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    \"What models we already tried training?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, it appears that the following features were one-hot encoded:\n",
      "\n",
      "* Age\n",
      "\n",
      "This information can be found in Document 3, which mentions that \"**Continous Features in the dataset: Age**\".\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    \"What features were one-hot encoded?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, it appears that the following data preprocessing steps were done:\n",
      "\n",
      "1. Removing redundant features: The document mentions that there may be many redundant features in the dataset, and suggests removing them. However, no specific examples or details are provided.\n",
      "2. Converting features into suitable form for modeling: The document mentions that some features may need to be converted into a more suitable form for modeling. Again, no specific examples or details are provided.\n",
      "3. Replacing null values with mean or median: The document mentions replacing null values with the mean or median age of the dataset. This is done to handle missing data.\n",
      "4. Grouping features: The document mentions grouping features, but no further details are provided.\n",
      "\n",
      "References:\n",
      "\n",
      "* Source document file name: notebooks/eda-to-prediction-dietanic.ipynb\n",
      "* Start index: 1595, 20173, 8721, 20811\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    \"Describe what data preprocessing was done?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, we tried hyperparameter tuning in the tutorial. According to the source documents, we tried tuning the hyperparameters for the two best classifiers, SVM and RandomForests. The tuning process was done using the Automated Hyperparameter Tuning tutorial provided by TensorFlow. The reference to the source document file name is \"notebooks/eda-to-prediction-dietanic.ipynb\" at line 36890.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    \"Did we try any hyperparameters tuning?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the code used to train a `GradientBoostedTreesModel` is located in the following document:\n",
      "\n",
      "Document(page_content='\\'code\\' cell: \\'[\\'model = tfdf.keras.GradientBoostedTreesModel(\\\\n\\', ...')\n",
      "\n",
      "This code trains a `GradientBoostedTreesModel` with the following parameters:\n",
      "\n",
      "* `verbose`: 0 (very few logs)\n",
      "* `features`: a list of feature usages, where each usage is represented by a `FeatureUsage` object\n",
      "* `exclude_non_specified_features`: True (only use the features in \"features\")\n",
      "* `random_seed`: 1234\n",
      "\n",
      "The model is then fit to the training data using the `fit()` method, and the evaluation metric (`accuracy` and `loss`) are printed.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    \"give me a code we used to train GradientBoostedTreesModel\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the code used to train a Gradient Boosted Trees Model is located in the following document:\n",
      "\n",
      "Document(page_content=\"'code' cell: '[\\'model = tfdf.keras.GradientBoostedTreesModel(\\\\n\\', ...\")\n",
      "\n",
      "This code defines the Gradient Boosted Trees Model using the `tfdf.keras` module, and then trains it on the training data using the `fit()` method. The `random_seed` parameter is set to 1234 for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    \"give me a code we used to train GradientBoostedTreesModel\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
